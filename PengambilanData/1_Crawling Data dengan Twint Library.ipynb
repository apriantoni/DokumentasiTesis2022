{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Crawling_data_From_Twitter.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NN1nUgRHsDiS"},"source":["# Scraping Data Twitter"]},{"cell_type":"markdown","metadata":{"id":"kbr5PTqFsbG_"},"source":["# Library yang Dibutuhkan"]},{"cell_type":"markdown","metadata":{"id":"Pa4ToxVJsidv"},"source":["Dalam melakukan proses scraping data dari Twitter digunakan beberapa library berikut:\n","\n","1. Library Twint : Digunakan untuk melakukan proses scraping data dari twitter. \n","Dapat dilihat di tautan https://pypi.org/project/twint/\n","2. Library Nest_Asyncio : Digunakan untuk menangaani error saat proses looping ketika proses scraping data dari Twitter. \n","Dapat dilihat di tautan https://pypi.org/project/nest-asyncio/\n","3. Library PyMySQL : Digunakan untuk meghubungkan dengan database MySQL agar dapat menyimpan data hasil scraping Twitter ke bentuk SQL\n","Dapat dilihat di tautan : https://pypi.org/project/PyMySQL/"]},{"cell_type":"markdown","metadata":{"id":"A48PdlhStixv"},"source":["**Proses install Library Twint**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yY-hLeJVwh8m","executionInfo":{"status":"ok","timestamp":1634221091679,"user_tz":-420,"elapsed":18339,"user":{"displayName":"Apriantoni Apriantoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgctGIwQY9tvNhxYPKOd4nXltDtglCE6sOxyEd71g=s64","userId":"10051271761277790344"}},"outputId":"a298891f-1adf-46a1-8c32-248f4ba6d5e9"},"source":["!pip install twint \n","#!pip install --user --upgrade git+https://github.com/yunusemrecatalcam/twint.git@twitter_legacy2\n","!pip install --user --upgrade -e git+https://github.com/twintproject/twint.git@origin/master#egg=twint"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting twint\n","  Downloading twint-2.1.20.tar.gz (31 kB)\n","Collecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 12.1 MB/s \n","\u001b[?25hCollecting aiodns\n","  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from twint) (4.6.3)\n","Collecting cchardet\n","  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n","\u001b[K     |████████████████████████████████| 263 kB 52.6 MB/s \n","\u001b[?25hCollecting elasticsearch\n","  Downloading elasticsearch-7.15.0-py2.py3-none-any.whl (378 kB)\n","\u001b[K     |████████████████████████████████| 378 kB 56.6 MB/s \n","\u001b[?25hRequirement already satisfied: pysocks in /usr/local/lib/python3.7/dist-packages (from twint) (1.7.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from twint) (1.1.5)\n","Collecting aiohttp_socks\n","  Downloading aiohttp_socks-0.6.0-py3-none-any.whl (9.2 kB)\n","Collecting schedule\n","  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: geopy in /usr/local/lib/python3.7/dist-packages (from twint) (1.17.0)\n","Collecting fake-useragent\n","  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n","Collecting googletransx\n","  Downloading googletransx-2.4.2.tar.gz (13 kB)\n","Collecting pycares>=4.0.0\n","  Downloading pycares-4.0.0-cp37-cp37m-manylinux2010_x86_64.whl (291 kB)\n","\u001b[K     |████████████████████████████████| 291 kB 56.0 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pycares>=4.0.0->aiodns->twint) (1.14.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->twint) (2.20)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 70.3 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (21.2.0)\n","Collecting async-timeout<4.0,>=3.0\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (3.0.4)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (3.7.4.3)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 49.8 MB/s \n","\u001b[?25hRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.10)\n","Collecting python-socks[asyncio]>=1.2.2\n","  Downloading python_socks-1.2.4-py3-none-any.whl (35 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch->twint) (2021.5.30)\n","Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch->twint) (1.24.3)\n","Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.7/dist-packages (from geopy->twint) (1.52)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from googletransx->twint) (2.23.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (2018.9)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->twint) (1.15.0)\n","Building wheels for collected packages: twint, fake-useragent, googletransx\n","  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for twint: filename=twint-2.1.20-py3-none-any.whl size=33930 sha256=ff47f9d5206bcec804d564d4db762d1ca4e0227f037d1ac8abff2d29f3d2d1e2\n","  Stored in directory: /root/.cache/pip/wheels/44/fc/77/99887a36b5c265a87516158858697d1a0b8f32c4d4dbddbb24\n","  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=b5c60370cf96b104364c3695c8cbbe37c5bd73cb4eaee89298103d992644e64c\n","  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n","  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletransx: filename=googletransx-2.4.2-py3-none-any.whl size=15968 sha256=dfe1ebcc3184fcd3d8d05fa1052354b08dca3b754134b30f0c1ac2c2bc4b52f9\n","  Stored in directory: /root/.cache/pip/wheels/66/d5/b1/31104b338f7fd45aa8f7d22587765db06773b13df48a89735f\n","Successfully built twint fake-useragent googletransx\n","Installing collected packages: multidict, yarl, python-socks, async-timeout, pycares, aiohttp, schedule, googletransx, fake-useragent, elasticsearch, cchardet, aiohttp-socks, aiodns, twint\n","Successfully installed aiodns-3.0.0 aiohttp-3.7.4.post0 aiohttp-socks-0.6.0 async-timeout-3.0.1 cchardet-2.1.7 elasticsearch-7.15.0 fake-useragent-0.1.11 googletransx-2.4.2 multidict-5.2.0 pycares-4.0.0 python-socks-1.2.4 schedule-1.1.0 twint-2.1.20 yarl-1.7.0\n","Obtaining twint from git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n","  Cloning https://github.com/twintproject/twint.git (to revision origin/master) to ./src/twint\n","  Running command git clone -q https://github.com/twintproject/twint.git /content/src/twint\n","\u001b[33m  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\u001b[0m\n","  Running command git checkout -q origin/master\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from twint) (3.7.4.post0)\n","Requirement already satisfied: aiodns in /usr/local/lib/python3.7/dist-packages (from twint) (3.0.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from twint) (4.6.3)\n","Requirement already satisfied: cchardet in /usr/local/lib/python3.7/dist-packages (from twint) (2.1.7)\n","Collecting dataclasses\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: elasticsearch in /usr/local/lib/python3.7/dist-packages (from twint) (7.15.0)\n","Requirement already satisfied: pysocks in /usr/local/lib/python3.7/dist-packages (from twint) (1.7.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from twint) (1.1.5)\n","Requirement already satisfied: aiohttp_socks in /usr/local/lib/python3.7/dist-packages (from twint) (0.6.0)\n","Requirement already satisfied: schedule in /usr/local/lib/python3.7/dist-packages (from twint) (1.1.0)\n","Requirement already satisfied: geopy in /usr/local/lib/python3.7/dist-packages (from twint) (1.17.0)\n","Requirement already satisfied: fake-useragent in /usr/local/lib/python3.7/dist-packages (from twint) (0.1.11)\n","Requirement already satisfied: googletransx in /usr/local/lib/python3.7/dist-packages (from twint) (2.4.2)\n","Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from aiodns->twint) (4.0.0)\n","Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pycares>=4.0.0->aiodns->twint) (1.14.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->twint) (2.20)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (3.7.4.3)\n","Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (3.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (1.7.0)\n","Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (3.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (21.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->twint) (5.2.0)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.10)\n","Requirement already satisfied: python-socks[asyncio]>=1.2.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp_socks->twint) (1.2.4)\n","Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch->twint) (1.24.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch->twint) (2021.5.30)\n","Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.7/dist-packages (from geopy->twint) (1.52)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from googletransx->twint) (2.23.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (2018.9)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->twint) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->twint) (1.15.0)\n","Installing collected packages: dataclasses, twint\n","  Running setup.py develop for twint\n","Successfully installed dataclasses-0.6 twint\n"]}]},{"cell_type":"markdown","metadata":{"id":"SFbROU5Xtpwy"},"source":["**Proses install Library Nest_asyncio**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lWIn2jnatvm2","executionInfo":{"status":"ok","timestamp":1634221099031,"user_tz":-420,"elapsed":3048,"user":{"displayName":"Apriantoni Apriantoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgctGIwQY9tvNhxYPKOd4nXltDtglCE6sOxyEd71g=s64","userId":"10051271761277790344"}},"outputId":"6864f0b1-011b-4f4c-d9da-fdabfc7c8376"},"source":["!pip install nest_asyncio"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"WOgEDNzCXsq6"},"source":["**Proses install Library PyMysql**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fbbq5lfFXyMz","executionInfo":{"status":"ok","timestamp":1634221110921,"user_tz":-420,"elapsed":8951,"user":{"displayName":"Apriantoni Apriantoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgctGIwQY9tvNhxYPKOd4nXltDtglCE6sOxyEd71g=s64","userId":"10051271761277790344"}},"outputId":"90662e81-be5a-4017-ef60-771723a53472"},"source":["!pip install pymysql\n","!pip install mysql-connector-python-rf"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pymysql\n","  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n","\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 30 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 1.4 MB/s \n","\u001b[?25hInstalling collected packages: pymysql\n","Successfully installed pymysql-1.0.2\n","Collecting mysql-connector-python-rf\n","  Downloading mysql-connector-python-rf-2.2.2.tar.gz (11.9 MB)\n","\u001b[K     |████████████████████████████████| 11.9 MB 188 kB/s \n","\u001b[?25hBuilding wheels for collected packages: mysql-connector-python-rf\n","  Building wheel for mysql-connector-python-rf (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mysql-connector-python-rf: filename=mysql_connector_python_rf-2.2.2-cp37-cp37m-linux_x86_64.whl size=249476 sha256=be3880637227bdce89b1b99cee22d2bf1312bcbafe669dd5f2a08330da0e34c6\n","  Stored in directory: /root/.cache/pip/wheels/68/59/cf/3b03557b26b4c75af3788a553e0ff9cf0b37a22d0c9cb01979\n","Successfully built mysql-connector-python-rf\n","Installing collected packages: mysql-connector-python-rf\n","Successfully installed mysql-connector-python-rf-2.2.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"62zrAeaP6RW7"},"source":["# Proses Scraping Data Twitter"]},{"cell_type":"markdown","metadata":{"id":"jxmXpR-GuG9O"},"source":["**Proses Import Library**"]},{"cell_type":"markdown","metadata":{"id":"D4SGA3zUuQVo"},"source":["Pertama, lakukan import library yang dibutuhkan berikut:"]},{"cell_type":"code","metadata":{"id":"6pMIHuauMNf2"},"source":["#import libraries\n","import twint\n","import nest_asyncio \n","import os\n","import pandas as pd\n","import numpy as np\n","import glob"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rT9NM5njuf2O"},"source":["Setelah itu, lakukan penanganan untuk mengatasi error looping saat melakukan proses scraping data dengan library **nest_asyncio**"]},{"cell_type":"code","metadata":{"id":"rkh_xagLuLU4"},"source":["#eksekusi library nest_asyncio\n","nest_asyncio.apply()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"whtMYNqi-32T"},"source":["Buat folder \"data\" untuk menyimpan hasil scraping data Twitter nantinya"]},{"cell_type":"code","metadata":{"id":"BKWNghqIJnYl"},"source":["#create folder data before covid-19\n","if not os.path.exists('data/noncovid'):\n","    os.makedirs('data/noncovid')\n","    \n","#create folder data during covid-19\n","if not os.path.exists('data/covid'):\n","    os.makedirs('data/covid')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"apOthfeUIANF"},"source":["Deklarasi function untuk konversi tuple ke string"]},{"cell_type":"code","metadata":{"id":"Uzrt_o6CE7lK"},"source":["def convertTuple(string): \n","    str =  ''.join(string) \n","    return str"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sq98gojsulV5"},"source":["**Konfigurasi dan proses scraping data dari Twitter**"]},{"cell_type":"markdown","metadata":{"id":"zGNktxpbuuuW"},"source":["Ada beberapa konfigurasi yang perlu dilakukan untuk melakukan proses scraping datanya, yaitu:\n","1. Mengatur **Tanggal Postingan Tweet** melalui parameter **Since dan Until** untuk mendapatkan data berdasarkan periodik waktu tertentu\n","2. Menampilkan statistik postingan melalui parameter **Stats dan Count** untuk menampilkan jumlah postingan, komen dan like \n","3. Mengatur Geocode wilayah yang dijadikan target lokasi melalui parameter **Geo** untuk menentukan lokasi spesifik dari data twitter yang akan di-scraping. defenisikan juga jarak radius yang membatasi proses pengambilan datanya.\n","4. Mengatur periodik data, dimana kondisi data dibagi ke dalam dua jenis yaitu data yang diambil saat masa pandemi COVID-19 dan sebelum terjadinya COVID-19"]},{"cell_type":"code","metadata":{"id":"jKD1egpPMW5P"},"source":["def general_twitter_scraping(start_date, until_date, latitude, longitude, radius, condition):\n","    \n","    #set geocode parameter value from latitude, longitude and radius\n","    geocode     = latitude,',',longitude,',',radius\n","    geocode_str = convertTuple(geocode).strip()\n","    \n","    if(condition == 'noncovid'):\n","        output_file = convertTuple(('data/noncovid/general_scrapping_result_',(start_date.replace('-', '')),'_',(until_date.replace('-', '')),'.csv')).strip()\n","    else:\n","        output_file = convertTuple(('data/covid/general_scrapping_result_',(start_date.replace('-', '')),'_',(until_date.replace('-', '')),'.csv')).strip()\n","        \n","    #configuration\n","    config = twint.Config()\n","    #config.Limit = 100\n","    config.Since = start_date\n","    config.Until = until_date\n","    config.Stats = True\n","    config.Count = True\n","    config.Favorites = True\n","    config.Retweets = True \n","    config.Store_csv = True\n","    config.Hide_output = True\n","    config.Include_retweets = True\n","    config.Geo = geocode_str\n","    config.Output = output_file\n","\n","    #running search\n","    twint.run.Search(config)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BHGH4l4I9ZuL"},"source":["Melakukan deklarasi variabel untuk mendefenisikan parameter yang digunakan dalam proses scraping data Twitter"]},{"cell_type":"code","metadata":{"id":"7C3NOL01EcL4"},"source":["latitude    = '-6.216657128974757'\n","longitude   = '106.83030289065285'\n","radius      = '10km'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"on4WMpA_IGLO"},"source":["Pemanggilan function general_twitter_scraping() untuk melakukan proses scraping data dari Twitter saat sebelum pandemi COVID-19"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l4UiDidCHSer","executionInfo":{"status":"ok","timestamp":1634221284945,"user_tz":-420,"elapsed":3108,"user":{"displayName":"Apriantoni Apriantoni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgctGIwQY9tvNhxYPKOd4nXltDtglCE6sOxyEd71g=s64","userId":"10051271761277790344"}},"outputId":"dd157f7a-3d1c-482a-a434-3950588f6ba5"},"source":["general_twitter_scraping('2021-10-11','2021-10-14',latitude,longitude,radius,'covid')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[+] Finished: Successfully collected 119 Tweets.\n"]}]},{"cell_type":"markdown","metadata":{"id":"a7akiNR29ogu"},"source":["Pemanggilan function general_twitter_scraping() untuk melakukan proses scraping data dari Twitter saat pandemi COVID-19"]},{"cell_type":"code","metadata":{"id":"lTzw0uMN95Mm"},"source":["general_twitter_scraping('2020-03-14','2020-06-14',latitude,longitude,radius,'covid')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KmcgB40d-gky"},"source":["Membaca file CSV yang berisi scraping data Twitter pada periode sebelum terjadi COVID-19"]},{"cell_type":"code","metadata":{"id":"5z7OC5A-HSet"},"source":["tweet_data_before_covid_df = pd.DataFrame()\n","\n","for files in glob.glob(\"data/noncovid/*.csv\"):\n","    tweet_data_before_covid_files = pd.read_csv(files)\n","    tweet_data_before_covid_df = tweet_data_before_covid_df.append(tweet_data_before_covid_files)\n","    \n","#menghapus data rows yang redudant\n","tweet_data_before_covid_df = pd.DataFrame.drop_duplicates(tweet_data_before_covid_df)\n","#tweet_data_before_covid_df = tweet_data_before_covid_df[~tweet_data_before_covid_df.index.duplicated(keep='first')]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YAl0mr6n-kWL"},"source":["Membaca file CSV yang berisi scraping data Twitter pada periode saat terjadi COVID-19"]},{"cell_type":"code","metadata":{"id":"XeDhIjasHSet"},"source":["tweet_data_during_covid_df = pd.DataFrame()\n","\n","for files in glob.glob(\"data/covid/*.csv\"):\n","    tweet_data_during_covid_files = pd.read_csv(files)\n","    tweet_data_during_covid_df = tweet_data_during_covid_df.append(tweet_data_during_covid_files)\n","    \n","#menghapus data rows yang redudant\n","tweet_data_during_covid_df = pd.DataFrame.drop_duplicates(tweet_data_during_covid_df)\n","#tweet_data_during_covid_df = tweet_data_during_covid_df[~tweet_data_during_covid_df.index.duplicated(keep='first')]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4_sJ5fLHSet","outputId":"3f8aedb1-4662-4404-d92b-6d549d88a6e6"},"source":["#before covid\n","number_of_rows_before_covid = len(tweet_data_before_covid_df.index)\n","print('Jumlah baris sebelum COVID-19: ',number_of_rows_before_covid)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Jumlah baris sebelum COVID-19:  652345\n"]}]},{"cell_type":"code","metadata":{"id":"1YAFaC-oHSeu","outputId":"129e6052-1c80-475f-b66b-9e8adb55e6e6"},"source":["#during covid\n","number_of_rows_during_covid = len(tweet_data_during_covid_df.index)\n","print('Jumlah baris saat COVID-19: ', number_of_rows_during_covid)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Jumlah baris saat COVID-19:  658490\n"]}]},{"cell_type":"code","metadata":{"id":"jFSbght9HSeu"},"source":["#all_tweets_data_df = pd.concat([tweet_data_before_covid_df, tweet_data_during_covid_df], axis=1, join='outer')\n","#all_tweets_data_df = pd.DataFrame(all_tweets_data_df);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4g20-wLNHSeu"},"source":["Melihat beberapa data sebelum terjadinya pandemi COVID-19"]},{"cell_type":"code","metadata":{"id":"2jJDfQr_HSev","outputId":"b1405e20-1622-4d5e-d086-ebed8fa47721"},"source":["tweet_data_before_covid_df.head(5)"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>conversation_id</th>\n","      <th>created_at</th>\n","      <th>date</th>\n","      <th>time</th>\n","      <th>timezone</th>\n","      <th>user_id</th>\n","      <th>username</th>\n","      <th>name</th>\n","      <th>place</th>\n","      <th>...</th>\n","      <th>geo</th>\n","      <th>source</th>\n","      <th>user_rt_id</th>\n","      <th>user_rt</th>\n","      <th>retweet_id</th>\n","      <th>reply_to</th>\n","      <th>retweet_date</th>\n","      <th>translate</th>\n","      <th>trans_src</th>\n","      <th>trans_dest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1219771501609570305</td>\n","      <td>1219771501609570305</td>\n","      <td>2020-01-21 23:59:22 UTC</td>\n","      <td>2020-01-21</td>\n","      <td>23:59:22</td>\n","      <td>0</td>\n","      <td>56072466</td>\n","      <td>harigelita</td>\n","      <td>Dian Mochtar</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1219771450304786432</td>\n","      <td>1219439859128922112</td>\n","      <td>2020-01-21 23:59:10 UTC</td>\n","      <td>2020-01-21</td>\n","      <td>23:59:10</td>\n","      <td>0</td>\n","      <td>239827943</td>\n","      <td>1dg4fux</td>\n","      <td>Aryawipu</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[{'screen_name': 'Tospotato', 'name': 'Kamisam...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1219771440943091712</td>\n","      <td>1219763519119220736</td>\n","      <td>2020-01-21 23:59:08 UTC</td>\n","      <td>2020-01-21</td>\n","      <td>23:59:08</td>\n","      <td>0</td>\n","      <td>80840926</td>\n","      <td>iwanneh</td>\n","      <td>.:::.</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[{'screen_name': 'NotesofMila', 'name': 'Mila🇮...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1219771347020083200</td>\n","      <td>1219771347020083200</td>\n","      <td>2020-01-21 23:58:46 UTC</td>\n","      <td>2020-01-21</td>\n","      <td>23:58:46</td>\n","      <td>0</td>\n","      <td>195299889</td>\n","      <td>bys_king11</td>\n","      <td>BODDAH</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1219771311368523776</td>\n","      <td>1219771311368523776</td>\n","      <td>2020-01-21 23:58:37 UTC</td>\n","      <td>2020-01-21</td>\n","      <td>23:58:37</td>\n","      <td>0</td>\n","      <td>406069088</td>\n","      <td>anggi_gunawan25</td>\n","      <td>Anggi Gunawan S.Tr.Sos</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 36 columns</p>\n","</div>"],"text/plain":["                    id      conversation_id               created_at  \\\n","0  1219771501609570305  1219771501609570305  2020-01-21 23:59:22 UTC   \n","1  1219771450304786432  1219439859128922112  2020-01-21 23:59:10 UTC   \n","2  1219771440943091712  1219763519119220736  2020-01-21 23:59:08 UTC   \n","3  1219771347020083200  1219771347020083200  2020-01-21 23:58:46 UTC   \n","4  1219771311368523776  1219771311368523776  2020-01-21 23:58:37 UTC   \n","\n","         date      time  timezone    user_id         username  \\\n","0  2020-01-21  23:59:22         0   56072466       harigelita   \n","1  2020-01-21  23:59:10         0  239827943          1dg4fux   \n","2  2020-01-21  23:59:08         0   80840926          iwanneh   \n","3  2020-01-21  23:58:46         0  195299889       bys_king11   \n","4  2020-01-21  23:58:37         0  406069088  anggi_gunawan25   \n","\n","                     name place  ...  \\\n","0            Dian Mochtar   NaN  ...   \n","1                Aryawipu   NaN  ...   \n","2                   .:::.   NaN  ...   \n","3                  BODDAH   NaN  ...   \n","4  Anggi Gunawan S.Tr.Sos   NaN  ...   \n","\n","                                          geo source user_rt_id user_rt  \\\n","0  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","1  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","2  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","3  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","4  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","\n","  retweet_id                                           reply_to  retweet_date  \\\n","0        NaN                                                 []           NaN   \n","1        NaN  [{'screen_name': 'Tospotato', 'name': 'Kamisam...           NaN   \n","2        NaN  [{'screen_name': 'NotesofMila', 'name': 'Mila🇮...           NaN   \n","3        NaN                                                 []           NaN   \n","4        NaN                                                 []           NaN   \n","\n","   translate trans_src trans_dest  \n","0        NaN       NaN        NaN  \n","1        NaN       NaN        NaN  \n","2        NaN       NaN        NaN  \n","3        NaN       NaN        NaN  \n","4        NaN       NaN        NaN  \n","\n","[5 rows x 36 columns]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"HiKzlexhHSev"},"source":["Melihat beberapa data saat terjadinya pandemi COVID-19"]},{"cell_type":"code","metadata":{"id":"stC5ko2iHSev","outputId":"3298b56f-7c2d-42c4-8340-95942464e544"},"source":["tweet_data_during_covid_df.head(5)"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>conversation_id</th>\n","      <th>created_at</th>\n","      <th>date</th>\n","      <th>time</th>\n","      <th>timezone</th>\n","      <th>user_id</th>\n","      <th>username</th>\n","      <th>name</th>\n","      <th>place</th>\n","      <th>...</th>\n","      <th>geo</th>\n","      <th>source</th>\n","      <th>user_rt_id</th>\n","      <th>user_rt</th>\n","      <th>retweet_id</th>\n","      <th>reply_to</th>\n","      <th>retweet_date</th>\n","      <th>translate</th>\n","      <th>trans_src</th>\n","      <th>trans_dest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1252748884301889538</td>\n","      <td>1252733407282388993</td>\n","      <td>2020-04-21 23:59:43 UTC</td>\n","      <td>2020-04-21</td>\n","      <td>23:59:43</td>\n","      <td>0</td>\n","      <td>134710864</td>\n","      <td>budikotelawala</td>\n","      <td>budi</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[{'screen_name': 'queennisme', 'name': 'queenn...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1252748846469283841</td>\n","      <td>1252748846469283841</td>\n","      <td>2020-04-21 23:59:34 UTC</td>\n","      <td>2020-04-21</td>\n","      <td>23:59:34</td>\n","      <td>0</td>\n","      <td>44393016</td>\n","      <td>kembletwitt</td>\n","      <td>Kemble</td>\n","      <td>{'type': 'Point', 'coordinates': [-6.1803, 106...</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1252748818405154817</td>\n","      <td>1252748818405154817</td>\n","      <td>2020-04-21 23:59:27 UTC</td>\n","      <td>2020-04-21</td>\n","      <td>23:59:27</td>\n","      <td>0</td>\n","      <td>924225714982809600</td>\n","      <td>anitatrisia8</td>\n","      <td>Anita Ita</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1252748804723376128</td>\n","      <td>1252748804723376128</td>\n","      <td>2020-04-21 23:59:24 UTC</td>\n","      <td>2020-04-21</td>\n","      <td>23:59:24</td>\n","      <td>0</td>\n","      <td>42618344</td>\n","      <td>adhis_lawliet</td>\n","      <td>adhisty puji</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1252748799497261058</td>\n","      <td>1252748799497261058</td>\n","      <td>2020-04-21 23:59:23 UTC</td>\n","      <td>2020-04-21</td>\n","      <td>23:59:23</td>\n","      <td>0</td>\n","      <td>1041540877</td>\n","      <td>abdillahrifki12</td>\n","      <td>imut</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 36 columns</p>\n","</div>"],"text/plain":["                    id      conversation_id               created_at  \\\n","0  1252748884301889538  1252733407282388993  2020-04-21 23:59:43 UTC   \n","1  1252748846469283841  1252748846469283841  2020-04-21 23:59:34 UTC   \n","2  1252748818405154817  1252748818405154817  2020-04-21 23:59:27 UTC   \n","3  1252748804723376128  1252748804723376128  2020-04-21 23:59:24 UTC   \n","4  1252748799497261058  1252748799497261058  2020-04-21 23:59:23 UTC   \n","\n","         date      time  timezone             user_id         username  \\\n","0  2020-04-21  23:59:43         0           134710864   budikotelawala   \n","1  2020-04-21  23:59:34         0            44393016      kembletwitt   \n","2  2020-04-21  23:59:27         0  924225714982809600     anitatrisia8   \n","3  2020-04-21  23:59:24         0            42618344    adhis_lawliet   \n","4  2020-04-21  23:59:23         0          1041540877  abdillahrifki12   \n","\n","           name                                              place  ...  \\\n","0          budi                                                NaN  ...   \n","1        Kemble  {'type': 'Point', 'coordinates': [-6.1803, 106...  ...   \n","2     Anita Ita                                                NaN  ...   \n","3  adhisty puji                                                NaN  ...   \n","4          imut                                                NaN  ...   \n","\n","                                          geo source user_rt_id user_rt  \\\n","0  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","1  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","2  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","3  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","4  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","\n","  retweet_id                                           reply_to  retweet_date  \\\n","0        NaN  [{'screen_name': 'queennisme', 'name': 'queenn...           NaN   \n","1        NaN                                                 []           NaN   \n","2        NaN                                                 []           NaN   \n","3        NaN                                                 []           NaN   \n","4        NaN                                                 []           NaN   \n","\n","   translate trans_src trans_dest  \n","0        NaN       NaN        NaN  \n","1        NaN       NaN        NaN  \n","2        NaN       NaN        NaN  \n","3        NaN       NaN        NaN  \n","4        NaN       NaN        NaN  \n","\n","[5 rows x 36 columns]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"LhSHGggvVVyF"},"source":["Proses scraping data yang dilakukan sebelumnya hanya mengambil data original postingan yang dilakukan oleh masing-masing pengguna, tetapi tidak mengambil juga data aktivitas Retweet yang dilakukan oleh pengguna. Data retweet ini hanya bisa di ambil melalui scrapping data ke masing-masing Timenline pengguna. Berikut ini mekanisme lain untuk melakukan scrapping data untuk mendapatkan data Retweet tersebut."]},{"cell_type":"code","metadata":{"id":"T59Ej6LUZADf"},"source":["def profile_twitter_scraping(username, start_date, until_date, latitude, longitude, radius):\n","    \n","    #set geocode parameter value from latitude, longitude and radius\n","    geocode = latitude,',',longitude,',',radius\n","    geocode_str = convertTuple(geocode).strip()\n","\n","    #configuration\n","    config = twint.Config()\n","    config.Username = username \n","    config.Since = start_date\n","    config.Until = until_date\n","    config.Stats = True\n","    config.Count = True\n","    config.Favorites = True\n","    #config.Profile_full = True\n","    #config.User_full = True\n","    #config.All = True\n","    #config.Store_csv = True\n","    #config.Include_retweets = True\n","    #config.Filter_retweets = \"include\"\n","    config.Filter_retweets = True\n","    config.Retweet = True\n","    #config.Hide_output = True\n","    config.Native_retweets = True\n","    config.Pandas = True\n","    config.Geo = geocode_str\n","    #config.Output = \"/content/data/profile_scraping_result.csv\"\n","\n","    #running search\n","    twint.run.Profile(config)\n","\n","    profile = twint.storage.panda.Tweets_df\n","\n","    return profile"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gouR_S1QHSew"},"source":["**Mengambil Data Retweet sebelum Pandemi COVID-19**"]},{"cell_type":"markdown","metadata":{"id":"37Og7zqeWH8z"},"source":["Deklarasi variabel **profile_before_covid** untuk menyimpan daftar nama pengguna Twitter yang melakukan aktifitas posting di periode sebelum COVID-19"]},{"cell_type":"code","metadata":{"id":"GR6T_uhfF2wm"},"source":["#reset index dataframe untuk membuat index nya unique\n","tweet_data_before_covid_df = tweet_data_before_covid_df.reset_index()\n","\n","#melakukan distinct username\n","profile_before_covid = pd.unique(tweet_data_before_covid_df['username'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R87N5Sa_HSew"},"source":["Melakukan penyimpanan data pengguna Twitter ke bentuk object DataFrame"]},{"cell_type":"code","metadata":{"id":"hce-OfBJHSex"},"source":["#menyimpan hasil distinct username pada dataframe\n","profile_before_covid = pd.DataFrame(profile_before_covid)\n","profile_before_covid = profile_before_covid.rename(columns={0: 'username'})\n","profile_before_covid = profile_before_covid.sort_values('username', ascending = 'True')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tgP2wWl9WOwE"},"source":["Melakukan looping ke masing-masing Timeline akun pengguna yang sudah di-scraping datanya untuk mendapatkan data aktivitas Retweet. Data yang digunakan adalah data saat belum terjadi pandemi COVID-19"]},{"cell_type":"code","metadata":{"id":"byyIECaNHSex","outputId":"90f76bd9-430b-4367-d063-0de9a65ff29b"},"source":["#deklarasi variabel untuk menyimpan data semua profile\n","all_profile_before_covid_df = pd.DataFrame()\n","all_profile_before_covid_df = all_profile_before_covid_df.drop(all_profile_before_covid_df.index, inplace=True)\n","\n","for index, row in profile_before_covid[100:10000].iterrows():\n","    del all_profile_before_covid_df\n","    all_profile_before_covid_df = pd.DataFrame()\n","    \n","    try:\n","        all_profile_before_covid_df = profile_twitter_scraping(row['username'],'2020-01-01','2020-02-08', latitude, longitude, radius)\n","\n","        if not all_profile_before_covid_df.empty:\n","            all_profile_before_covid_df = all_profile_before_covid_df.loc[all_profile_before_covid_df['retweet'] == True]\n","\n","            if not os.path.isfile(\"data/noncovid/profile_scraping_result_0_10000.csv\"):\n","                all_profile_before_covid_df.to_csv('data/noncovid/profile_scraping_result_0_10000.csv')\n","            else:\n","                all_profile_before_covid_df = all_profile_before_covid_df.append(all_profile_before_covid_df)\n","                all_profile_before_covid_df.to_csv('data/noncovid/profile_scraping_result_0_10000.csv', mode='a', index=True, header=False)\n","    except:\n","        pass"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["1225452226165923840 2020-02-06 23:12:33 +0700 <2137ath_> @GiaPratamaMD Saya ISFP-T , dok  https://t.co/K10PHZMytz | 0 replies 0 retweets 0 likes\n","1221027511192018946 2020-01-25 18:10:18 +0700 <2137ath_> Sy open mind thd semua jenis pengobatan, selama tdk bertentangan dg syariat. Tapi kenapa sih para pengobat tradisional itu sering bgt mempertentangkan metodenya dg kedokteran modern? Negatif thinking thd kedokteran modern terus. Ini yg bikin sy antipati | 0 replies 0 retweets 0 likes\n","1219291621910429697 2020-01-20 23:12:30 +0700 <2137ath_> @republikaonline Rajam sampai mati | 0 replies 0 retweets 0 likes\n","1215688087646494720 2020-01-11 00:33:21 +0700 <2137ath_> @GiaPratamaMD NO GOOGLING ALLOWED. Every answer must start with the first letter of your FIRST name.   WEAR - Niqab DRINK - Nabeez FOOD - Nasi ANIMAL - Ngengat PROFESSION - Nanny for my kids SOMETHING IN YOUR HOME - Nampan BODY PART - Nails  COPY, PASTE, &amp; HAVE FUN 😆 | 0 replies 0 retweets 0 likes\n","1215462365522644994 2020-01-10 09:36:24 +0700 <2137ath_> @dhanatini , | 1 replies 0 retweets 0 likes\n","1215080163861397505 2020-01-09 08:17:40 +0700 <2137ath_> RT @infoBMKG: UPDATE RELEASE BMKG Waspada potensi hujan sedang hingga lebat di wilayah jabodetabek periode tanggal 08-12 Januari 2020  https… | 0 replies 805 retweets 0 likes\n"]}]},{"cell_type":"markdown","metadata":{"id":"GPF6fd3fHSey"},"source":["Membaca file CSV yang berisi scraping data Twitter pada periode sebelum terjadi COVID-19"]},{"cell_type":"code","metadata":{"id":"rtthZvZqHSey"},"source":["#tweet_data_before_covid_df = tweet_data_before_covid_df.dropna(how='any',axis=0)\n","tweet_data_before_covid_df = pd.DataFrame()\n","\n","for files in glob.glob(\"data/noncovid/*.csv\"):\n","    tweet_data_before_covid_files = pd.read_csv(files)\n","    tweet_data_before_covid_df = tweet_data_before_covid_df.append(tweet_data_before_covid_files)\n","    \n","#menghapus data rows yang redudant\n","tweet_data_before_covid_df = pd.DataFrame.drop_duplicates(tweet_data_before_covid_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SWArHMWOHSez"},"source":["**Mengambil Data Retweet saat Pandemi COVID-19**"]},{"cell_type":"markdown","metadata":{"id":"L2TNtmcxHSez"},"source":["Deklarasi variabel **profile_during_covid** untuk menyimpan daftar nama pengguna Twitter yang melakukan aktifitas posting di periode sebelum COVID-19"]},{"cell_type":"code","metadata":{"id":"cQHlckBCHSez"},"source":["#reset index dataframe untuk membuat index nya unique\n","tweet_data_during_covid_df = tweet_data_during_covid_df.reset_index()\n","\n","#melakukan distinct username\n","profile_during_covid = pd.unique(tweet_data_during_covid_df['username'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mZ32iRaNHSez"},"source":["Melakukan penyimpanan data pengguna Twitter ke bentuk object DataFrame"]},{"cell_type":"code","metadata":{"id":"nUyUZ5KrHSe0"},"source":["#menyimpan hasil distinct username pada dataframe\n","profile_during_covid = pd.DataFrame(profile_during_covid)\n","profile_during_covid = profile_during_covid.rename(columns={0: 'username'})\n","profile_during_covid = profile_during_covid.sort_values('username', ascending = 'True')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3kgZFm5gHSe0"},"source":["Melakukan looping ke masing-masing Timeline akun pengguna yang sudah di-scraping datanya untuk mendapatkan data aktivitas Retweet. Data yang digunakan adalah data saat terjadi pandemi COVID-19"]},{"cell_type":"code","metadata":{"id":"PtpmkUv1HSe0"},"source":["#deklarasi variabel untuk menyimpan data semua profile\n","#all_profile_before_covid_df = all_profile_before_covid_df.drop(all_profile_before_covid_df.index, inplace=True)\n","\n","for index, row in profile_during_covid[0:1000].iterrows():\n","    del all_profile_during_covid_df\n","    all_profile_during_covid_df = pd.DataFrame()\n","    \n","    try:\n","        all_profile_during_covid_df = profile_twitter_scraping(row['username'],'2020-04-01','2020-05-08', latitude, longitude, radius)\n","\n","        if not all_profile_during_covid_df.empty:\n","            all_profile_during_covid_df = all_profile_during_covid_df.loc[all_profile_during_covid_df['retweet'] == True]\n","\n","            if not os.path.isfile(\"data/covid/profile_scraping_result_0_1000.csv\"):\n","                all_profile_during_covid_df.to_csv('data/covid/profile_scraping_result_0_1000.csv')\n","            else:\n","                all_profile_during_covid_df = all_profile_during_covid_df.append(all_profile_during_covid_df)\n","                all_profile_during_covid_df.to_csv('data/covid/profile_scraping_result_0_1000.csv', mode='a', index=True, header=False)\n","    except:\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ALNrrloiHSe0"},"source":["Membaca file CSV yang berisi scraping data Twitter pada periode saat terjadi COVID-19"]},{"cell_type":"code","metadata":{"id":"YLte5hOgHSe1"},"source":["tweet_data_during_covid_df = tweet_data_during_covid_df.dropna(how='any',axis=0)\n","tweet_data_during_covid_df = pd.DataFrame()\n","\n","for files in glob.glob(\"data/covid/*.csv\"):\n","    tweet_data_during_covid_files = pd.read_csv(files)\n","    tweet_data_during_covid_df = tweet_data_during_covid_df.append(tweet_data_during_covid_files)\n","    \n","#menghapus data rows yang redudant\n","tweet_data_during_covid_df = pd.DataFrame.drop_duplicates(tweet_data_during_covid_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKqPBUKsHSe1"},"source":["Menggabungkan data tweets sebelum dan saat COVID-19"]},{"cell_type":"code","metadata":{"id":"09WAyg2XHSe1"},"source":["all_tweets_data_df = pd.concat([tweet_data_before_covid_df, tweet_data_during_covid_df], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"27OIj_awHSe1"},"source":["Menghitung jumlah baris data keseluruhan"]},{"cell_type":"code","metadata":{"id":"oMhzhTbvHSe1","outputId":"4075a605-0d9a-4a90-b8c1-ef3714d5d935"},"source":["print('Jumlah baris keseluruhan: ', len(all_tweets_data_df.index))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Jumlah baris keseluruhan:  1310835\n"]}]},{"cell_type":"code","metadata":{"id":"IoCzJ6LHHSe1","outputId":"9afc2014-7d7c-4334-f30c-e86cd75d7042"},"source":["all_tweets_data_df.head(5)"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>conversation_id</th>\n","      <th>created_at</th>\n","      <th>date</th>\n","      <th>time</th>\n","      <th>timezone</th>\n","      <th>user_id</th>\n","      <th>username</th>\n","      <th>name</th>\n","      <th>place</th>\n","      <th>...</th>\n","      <th>geo</th>\n","      <th>source</th>\n","      <th>user_rt_id</th>\n","      <th>user_rt</th>\n","      <th>retweet_id</th>\n","      <th>reply_to</th>\n","      <th>retweet_date</th>\n","      <th>translate</th>\n","      <th>trans_src</th>\n","      <th>trans_dest</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1219771501609570305</td>\n","      <td>1219771501609570305</td>\n","      <td>2020-01-21 23:59:22 UTC</td>\n","      <td>2020-01-21</td>\n","      <td>23:59:22</td>\n","      <td>0</td>\n","      <td>56072466</td>\n","      <td>harigelita</td>\n","      <td>Dian Mochtar</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1219771450304786432</td>\n","      <td>1219439859128922112</td>\n","      <td>2020-01-21 23:59:10 UTC</td>\n","      <td>2020-01-21</td>\n","      <td>23:59:10</td>\n","      <td>0</td>\n","      <td>239827943</td>\n","      <td>1dg4fux</td>\n","      <td>Aryawipu</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[{'screen_name': 'Tospotato', 'name': 'Kamisam...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1219771440943091712</td>\n","      <td>1219763519119220736</td>\n","      <td>2020-01-21 23:59:08 UTC</td>\n","      <td>2020-01-21</td>\n","      <td>23:59:08</td>\n","      <td>0</td>\n","      <td>80840926</td>\n","      <td>iwanneh</td>\n","      <td>.:::.</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[{'screen_name': 'NotesofMila', 'name': 'Mila🇮...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1219771347020083200</td>\n","      <td>1219771347020083200</td>\n","      <td>2020-01-21 23:58:46 UTC</td>\n","      <td>2020-01-21</td>\n","      <td>23:58:46</td>\n","      <td>0</td>\n","      <td>195299889</td>\n","      <td>bys_king11</td>\n","      <td>BODDAH</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1219771311368523776</td>\n","      <td>1219771311368523776</td>\n","      <td>2020-01-21 23:58:37 UTC</td>\n","      <td>2020-01-21</td>\n","      <td>23:58:37</td>\n","      <td>0</td>\n","      <td>406069088</td>\n","      <td>anggi_gunawan25</td>\n","      <td>Anggi Gunawan S.Tr.Sos</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>-6.216657128974757,106.83030289065285,10km</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 36 columns</p>\n","</div>"],"text/plain":["                    id      conversation_id               created_at  \\\n","0  1219771501609570305  1219771501609570305  2020-01-21 23:59:22 UTC   \n","1  1219771450304786432  1219439859128922112  2020-01-21 23:59:10 UTC   \n","2  1219771440943091712  1219763519119220736  2020-01-21 23:59:08 UTC   \n","3  1219771347020083200  1219771347020083200  2020-01-21 23:58:46 UTC   \n","4  1219771311368523776  1219771311368523776  2020-01-21 23:58:37 UTC   \n","\n","         date      time  timezone    user_id         username  \\\n","0  2020-01-21  23:59:22         0   56072466       harigelita   \n","1  2020-01-21  23:59:10         0  239827943          1dg4fux   \n","2  2020-01-21  23:59:08         0   80840926          iwanneh   \n","3  2020-01-21  23:58:46         0  195299889       bys_king11   \n","4  2020-01-21  23:58:37         0  406069088  anggi_gunawan25   \n","\n","                     name place  ...  \\\n","0            Dian Mochtar   NaN  ...   \n","1                Aryawipu   NaN  ...   \n","2                   .:::.   NaN  ...   \n","3                  BODDAH   NaN  ...   \n","4  Anggi Gunawan S.Tr.Sos   NaN  ...   \n","\n","                                          geo source user_rt_id user_rt  \\\n","0  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","1  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","2  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","3  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","4  -6.216657128974757,106.83030289065285,10km    NaN        NaN     NaN   \n","\n","  retweet_id                                           reply_to  retweet_date  \\\n","0        NaN                                                 []           NaN   \n","1        NaN  [{'screen_name': 'Tospotato', 'name': 'Kamisam...           NaN   \n","2        NaN  [{'screen_name': 'NotesofMila', 'name': 'Mila🇮...           NaN   \n","3        NaN                                                 []           NaN   \n","4        NaN                                                 []           NaN   \n","\n","   translate trans_src trans_dest  \n","0        NaN       NaN        NaN  \n","1        NaN       NaN        NaN  \n","2        NaN       NaN        NaN  \n","3        NaN       NaN        NaN  \n","4        NaN       NaN        NaN  \n","\n","[5 rows x 36 columns]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"xuy4Wp1aHSe2"},"source":[""],"execution_count":null,"outputs":[]}]}