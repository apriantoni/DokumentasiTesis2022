{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN1nUgRHsDiS"
   },
   "source": [
    "# Scraping Data Twitter berdasarkan User yang Terseleksi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbr5PTqFsbG_"
   },
   "source": [
    "# Library yang Dibutuhkan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pa4ToxVJsidv"
   },
   "source": [
    "Dalam melakukan proses scraping data dari Twitter digunakan beberapa library berikut:\n",
    "\n",
    "1. Library Twint : Digunakan untuk melakukan proses scraping data dari twitter. \n",
    "Dapat dilihat di tautan https://pypi.org/project/twint/\n",
    "2. Library Nest_Asyncio : Digunakan untuk menangaani error saat proses looping ketika proses scraping data dari Twitter. \n",
    "Dapat dilihat di tautan https://pypi.org/project/nest-asyncio/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A48PdlhStixv"
   },
   "source": [
    "**Proses install Library Twint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yY-hLeJVwh8m",
    "outputId": "3f52bbf0-9f96-4287-d092-27506a57cac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: twint in ./src/twint (2.1.21)\n",
      "Requirement already satisfied: aiohttp in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (3.7.4.post0)\n",
      "Requirement already satisfied: aiodns in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (2.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (4.8.2)\n",
      "Requirement already satisfied: cchardet in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (2.1.7)\n",
      "Requirement already satisfied: dataclasses in /Users/toniafriantoni/.local/lib/python3.7/site-packages (from twint) (0.6)\n",
      "Requirement already satisfied: elasticsearch in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (7.11.0)\n",
      "Requirement already satisfied: pysocks in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (1.7.1)\n",
      "Requirement already satisfied: pandas in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (1.0.1)\n",
      "Requirement already satisfied: aiohttp_socks in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (0.6.0)\n",
      "Requirement already satisfied: schedule in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (1.0.0)\n",
      "Requirement already satisfied: geopy in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (2.1.0)\n",
      "Requirement already satisfied: fake-useragent in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (0.1.11)\n",
      "Requirement already satisfied: googletransx in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (2.4.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint) (19.3.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint) (3.7.4.3)\n",
      "Requirement already satisfied: pycares>=3.0.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiodns->twint) (3.1.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->twint) (1.9.5)\n",
      "Requirement already satisfied: certifi in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from elasticsearch->twint) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from elasticsearch->twint) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from pandas->twint) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from pandas->twint) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from pandas->twint) (2019.3)\n",
      "Requirement already satisfied: python-socks[asyncio]>=1.2.2 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp_socks->twint) (1.2.2)\n",
      "Requirement already satisfied: geographiclib<2,>=1.49 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from geopy->twint) (1.50)\n",
      "Requirement already satisfied: requests in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from googletransx->twint) (2.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.8)\n",
      "Requirement already satisfied: cffi>=1.5.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from pycares>=3.0.0->aiodns->twint) (1.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->twint) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from cffi>=1.5.0->pycares>=3.0.0->aiodns->twint) (2.19)\n",
      "Obtaining twint from git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
      "  Updating ./src/twint clone (to revision origin/master)\n",
      "  Running command git fetch -q --tags\n",
      "\u001b[33m  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\u001b[0m\n",
      "  Running command git reset --hard -q origin/master\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (3.7.4.post0)\n",
      "Requirement already satisfied, skipping upgrade: aiodns in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (4.8.2)\n",
      "Requirement already satisfied, skipping upgrade: cchardet in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (2.1.7)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses in /Users/toniafriantoni/.local/lib/python3.7/site-packages (from twint) (0.6)\n",
      "Requirement already satisfied, skipping upgrade: elasticsearch in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (7.11.0)\n",
      "Requirement already satisfied, skipping upgrade: pysocks in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp_socks in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: schedule in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: geopy in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: fake-useragent in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (0.1.11)\n",
      "Requirement already satisfied, skipping upgrade: googletransx in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from twint) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.5 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<5.0,>=2.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint) (5.1.0)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pycares>=3.0.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiodns->twint) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: soupsieve>=1.2 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->twint) (1.9.5)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<2,>=1.21.1 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from elasticsearch->twint) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from elasticsearch->twint) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from pandas->twint) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from pandas->twint) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from pandas->twint) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: python-socks[asyncio]>=1.2.2 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from aiohttp_socks->twint) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: geographiclib<2,>=1.49 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from geopy->twint) (1.50)\n",
      "Requirement already satisfied, skipping upgrade: requests in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from googletransx->twint) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: idna>=2.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp->twint) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.5.0 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from pycares>=3.0.0->aiodns->twint) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->twint) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (from cffi>=1.5.0->pycares>=3.0.0->aiodns->twint) (2.19)\n",
      "Installing collected packages: twint\n",
      "  Attempting uninstall: twint\n",
      "    Found existing installation: twint 2.1.21\n",
      "    Uninstalling twint-2.1.21:\n",
      "      Successfully uninstalled twint-2.1.21\n",
      "  Running setup.py develop for twint\n",
      "Successfully installed twint\n"
     ]
    }
   ],
   "source": [
    "!pip install twint \n",
    "#!pip install --user --upgrade git+https://github.com/yunusemrecatalcam/twint.git@twitter_legacy2\n",
    "!pip install --user --upgrade -e git+https://github.com/twintproject/twint.git@origin/master#egg=twint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFbROU5Xtpwy"
   },
   "source": [
    "**Proses install Library Nest_asyncio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lWIn2jnatvm2",
    "outputId": "1df321e9-b6ce-49a6-92fd-aa2e71488c71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62zrAeaP6RW7"
   },
   "source": [
    "# Proses Scraping Data Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxmXpR-GuG9O"
   },
   "source": [
    "**Proses Import Library**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4SGA3zUuQVo"
   },
   "source": [
    "Pertama, lakukan import library yang dibutuhkan berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "6pMIHuauMNf2",
    "outputId": "5040569a-5e9f-43cc-cbdd-a4a1226d437a"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import twint\n",
    "import nest_asyncio \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT9NM5njuf2O"
   },
   "source": [
    "Setelah itu, lakukan penanganan untuk mengatasi error looping saat melakukan proses scraping data dengan library **nest_asyncio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rkh_xagLuLU4"
   },
   "outputs": [],
   "source": [
    "#eksekusi library nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apOthfeUIANF"
   },
   "source": [
    "Deklarasi function untuk konversi tuple ke string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Uzrt_o6CE7lK"
   },
   "outputs": [],
   "source": [
    "def convertTuple(string): \n",
    "    str =  ''.join(string) \n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq98gojsulV5"
   },
   "source": [
    "**Konfigurasi dan proses scraping data dari Twitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7C3NOL01EcL4"
   },
   "outputs": [],
   "source": [
    "latitude    = '-6.216657128974757'\n",
    "longitude   = '106.83030289065285'\n",
    "radius      = '10km'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "on4WMpA_IGLO"
   },
   "source": [
    "Pemanggilan function general_twitter_scraping() untuk melakukan proses scraping data dari Twitter saat sebelum pandemi COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "T59Ej6LUZADf"
   },
   "outputs": [],
   "source": [
    "def profile_twitter_scraping(username, start_date, until_date, latitude, longitude, radius):\n",
    "    \n",
    "    #set geocode parameter value from latitude, longitude and radius\n",
    "    geocode = latitude,',',longitude,',',radius\n",
    "    geocode_str = convertTuple(geocode).strip()\n",
    "\n",
    "    #configuration\n",
    "    config = twint.Config()\n",
    "    config.Username = username \n",
    "    config.Since = start_date\n",
    "    config.Until = until_date\n",
    "    config.Stats = True\n",
    "    config.Count = True\n",
    "    config.Favorites = True\n",
    "    config.Profile_full = True\n",
    "    config.User_full = True\n",
    "    config.All = True\n",
    "    config.Store_csv = True\n",
    "    config.Include_retweets = True\n",
    "    config.Filter_retweets = \"include\"\n",
    "    config.Filter_retweets = True\n",
    "    config.Retweet = True\n",
    "    config.Hide_output = False\n",
    "    config.Native_retweets = True\n",
    "    config.Pandas = True\n",
    "    config.Geo = geocode_str\n",
    "    #config.Output = \"/content/data/profile_scraping_result.csv\"\n",
    "\n",
    "    #running search\n",
    "    twint.run.Profile(config)\n",
    "\n",
    "    profile = twint.storage.panda.Tweets_df\n",
    "\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mengambil Data Retweet sebelum Pandemi COVID-19**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membaca file CSV yang berisi data pengguna Twitter yang sudah terseleksi pada periode sebelum COVID-19 kemudian melakukan penyimpanan data ke bentuk object DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#membaca dan menyimpan username pada dataframe\n",
    "profile_before_covid_df = pd.read_csv('../data/filter/noncovid/user_after_filter_13122019-13032020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgP2wWl9WOwE"
   },
   "source": [
    "Melakukan looping ke masing-masing Timeline akun pengguna yang sudah di-scraping datanya untuk mendapatkan data aktivitas Retweet. Data yang digunakan adalah data saat belum terjadi pandemi COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deklarasi variabel untuk menyimpan data semua profile\n",
    "all_profile_before_covid_df = pd.DataFrame()\n",
    "all_profile_before_covid_df = all_profile_before_covid_df.drop(all_profile_before_covid_df.index, inplace=True)\n",
    "\n",
    "for index, row in profile_before_covid_df.iterrows():\n",
    "    del all_profile_before_covid_df\n",
    "    all_profile_before_covid_df = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        all_profile_before_covid_df = profile_twitter_scraping(row['username'],'2019-12-13','2020-03-14', latitude, longitude, radius)\n",
    "\n",
    "        if not all_profile_before_covid_df.empty:\n",
    "            all_profile_before_covid_df = all_profile_before_covid_df.loc[all_profile_before_covid_df['retweet'] == True]\n",
    "\n",
    "            if not os.path.isfile(\"../data/filter/noncovid/final_user_after_filter_13122019-13032020.csv\"):\n",
    "                all_profile_before_covid_df.to_csv('../data/filter/noncovid/final_user_after_filter_13122019-13032020.csv')\n",
    "            else:\n",
    "                all_profile_before_covid_df = all_profile_before_covid_df.append(all_profile_before_covid_df)\n",
    "                all_profile_before_covid_df.to_csv('../data/filter/noncovid/final_user_after_filter_13122019-13032020.csv', mode='a', index=True, header=False)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membaca file CSV yang berisi scraping data Twitter pada periode sebelum terjadi COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#membaca dan menyimpan hasil scrapping data\n",
    "profile_before_covid_df = pd.read_csv('data/filter/noncovid/final_user_after_filter_13122019-13032020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mengambil Data Retweet saat Pandemi COVID-19**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membaca file CSV yang berisi data pengguna Twitter yang sudah terseleksi pada periode sebelum COVID-19 kemudian melakukan penyimpanan data ke bentuk object DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#membaca dan menyimpan username pada dataframe\n",
    "profile_during_covid_df = pd.read_csv('../data/filter/noncovid/final_user_after_filter_13122019-13032020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan looping ke masing-masing Timeline akun pengguna yang sudah di-scraping datanya untuk mendapatkan data aktivitas Retweet. Data yang digunakan adalah data saat terjadi pandemi COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deklarasi variabel untuk menyimpan data semua profile\n",
    "all_profile_during_covid_df = pd.DataFrame()\n",
    "all_profile_during_covid_df = all_profile_during_covid_df.drop(all_profile_during_covid_df.index, inplace=True)\n",
    "\n",
    "for index, row in profile_during_covid_df.iterrows():\n",
    "    del all_profile_during_covid_df\n",
    "    all_profile_during_covid_df = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        all_profile_during_covid_df = profile_twitter_scraping(row['username'],'2020-03-14','2020-06-14', latitude, longitude, radius)\n",
    "\n",
    "        if not all_profile_during_covid_df.empty:\n",
    "            all_profile_during_covid_df = all_profile_during_covid_df.loc[all_profile_during_covid_df['retweet'] == True]\n",
    "\n",
    "            if not os.path.isfile(\"../data/filter/covid/final_user_after_filter_14032020-13062020.csv\"):\n",
    "                all_profile_during_covid_df.to_csv('../data/filter/covid/final_user_after_filter_14032020-13062020.csv')\n",
    "            else:\n",
    "                all_profile_during_covid_df = all_profile_during_covid_df.append(all_profile_during_covid_df)\n",
    "                all_profile_during_covid_df.to_csv('../data/filter/covid/final_user_after_filter_14032020-13062020.csv', mode='a', index=True, header=False)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membaca file CSV yang berisi scraping data Twitter pada periode saat terjadi COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#membaca hasil scrapping data\n",
    "profile_during_covid_df = pd.read_csv('../data/filter/covid/final_user_after_filter_14032020-13062020.csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
