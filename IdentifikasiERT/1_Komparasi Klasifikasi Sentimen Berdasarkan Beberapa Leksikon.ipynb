{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Task_5_Comparison_of_Lexicon-based_Classification.ipynb","provenance":[],"collapsed_sections":["dJeh1_KQF0o-","YcJuAT0Ax4Gz","zkPwgaDOIpHg","XcZTKpcoI5Ce","7EX8m07iPKea","pkiJ0heGRbSx","gQJTSh0jPCEZ"],"private_outputs":true,"machine_shape":"hm"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"EKFw8F18x4Gt"},"source":["# Klasifikasi Data Emosi berdasarkan Komparasi Beberapa Leksikon"]},{"cell_type":"markdown","source":["## Persiapan Data"],"metadata":{"id":"dJeh1_KQF0o-"}},{"cell_type":"markdown","source":["**Install Packages**"],"metadata":{"id":"3zs1zsaNRssJ"}},{"cell_type":"code","source":["!pip install deep-translator"],"metadata":{"id":"D1UTlcUYRusy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install swifter"],"metadata":{"id":"4InR20UWWt6E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install afinn"],"metadata":{"id":"BJqpdUnXQF3Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Import Libraries**"],"metadata":{"id":"atdXzM3bFt8F"}},{"cell_type":"code","metadata":{"id":"lfM0d_2dx4Gw"},"source":["import csv\n","import swifter\n","import nltk\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from deep_translator import GoogleTranslator\n","from sklearn.preprocessing import LabelEncoder\n","from nltk.tokenize import word_tokenize\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neural_network import MLPClassifier\n","from sklearn import svm\n","from scipy.sparse import hstack\n","from afinn import Afinn\n","from sklearn.metrics import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"id":"H9-1HVVHlnaG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7OVly9r2x4Gx"},"source":["**Koneksi Data ke Google Drive**"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"iiTB8AyKfdIF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jJihsxT5x4Gy"},"source":["**Import Data CSV**"]},{"cell_type":"code","metadata":{"id":"La5IxJldx4Gy"},"source":["#covid\n","#data_df_1 = pd.read_csv('/content/gdrive/MyDrive/Thesis/output/21122021_covid_ekstraksi_variabel_perilaku.csv')\n","#data_df_2 = pd.read_csv('/content/gdrive/MyDrive/Thesis/dataset/1-7333 after_covid_data.csv')\n","#data_df_2.loc[data_df_2.Sentiment == 'Positif', 'Sentiment'] = 'Positive'\n","\n","#noncovid\n","data_df_1 = pd.read_csv('/content/gdrive/MyDrive/Thesis/output/21122021_noncovid_ekstraksi_variabel_perilaku.csv')\n","data_df_2 = pd.read_csv('/content/gdrive/MyDrive/Thesis/dataset/7334-30980 before_covid_data.csv')\n","data_df_2.loc[data_df_2.Sentiment == 'positif', 'Sentiment'] = 'Positif'\n","data_df_2.loc[data_df_2.Sentiment == 'negative', 'Sentiment'] = 'Negative'\n","data_df_2.loc[data_df_2.Sentiment == 'Positif', 'Sentiment'] = 'Positive'\n","\n","\n","data_df = pd.DataFrame()\n","data_df['username'] = data_df_1['username'].copy()\n","data_df['tweet_char'] = data_df_1['tweet_char'].copy()\n","data_df['tweet_tokens_final'] = data_df_1['tweet_tokens_final'].copy()\n","data_df['Emotion'] = data_df_2['Emotion'].copy()\n","data_df['Sentiment'] = data_df_2['Sentiment'].copy()\n","\n","data_df['tweet_tokens_final'] = data_df['tweet_tokens_final'].str.replace(\"[\",\"\")\n","data_df['tweet_tokens_final'] = data_df['tweet_tokens_final'].str.replace(\"]\",\"\")\n","data_df['tweet_tokens_final'] = data_df['tweet_tokens_final'].str.replace(\"'\",\"\")\n","data_df['tweet_tokens_final'] = data_df['tweet_tokens_final'].str.replace(\" \",\"\")\n","data_df['tweet_tokens_final'] = data_df['tweet_tokens_final'].str.split(\",\")\n","data_df['tweet_tokens_final'] = [i for i in data_df['tweet_tokens_final'] if i != '']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menghitung jumlah data setiap sentimen (Positive, Neutral, dan Negative)"],"metadata":{"id":"w3My06tyGFks"}},{"cell_type":"code","source":["data_df['Sentiment'].value_counts()"],"metadata":{"id":"syDY1iytcTT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from sklearn.model_selection import StratifiedShuffleSplit\n","\n","#selected_data_df = data_df[['tweet', 'emotion']].copy()\n","\n","#X_train, X_test= train_test_split(selected_data_df, test_size=50, random_state = 99,\n","#                                          stratify=selected_data_df['emotion'])\n","\n","#X_test.to_csv('/content/gdrive/MyDrive/Thesis/covid_stratified.csv')"],"metadata":{"id":"crxAgOPM4ve5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#X_test['emotion'].value_counts()"],"metadata":{"id":"kbVcnd3lCzDX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#data_df"],"metadata":{"id":"9rOd34snX9jc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pemrosesan Data"],"metadata":{"id":"GglFpw8XGN6R"}},{"cell_type":"markdown","metadata":{"id":"YcJuAT0Ax4Gz"},"source":["### Proses Ekstraksi Fitur Unigram dan Bigram dengan TF-IDF"]},{"cell_type":"markdown","metadata":{"id":"AWditYBmx4Gz"},"source":["Melakukan split data training dan testing dengan perbandingan  80%:20%"]},{"cell_type":"code","metadata":{"id":"4PFctWjVx4Gz"},"source":["data_df = data_df[(data_df['Sentiment'] != 'Neutral')]\n","X = data_df['tweet_char']\n","y = data_df['Sentiment']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r13xT_HAx4G0"},"source":["Ekstraksi fitur Unigram"]},{"cell_type":"code","metadata":{"id":"QOCioajax4G0"},"source":["unigram_vectorizer = TfidfVectorizer(\n","    sublinear_tf=True,\n","    strip_accents='unicode',\n","    analyzer='word',\n","    token_pattern=r'\\w{1,}',\n","    ngram_range=(1, 1),\n","    max_features=100000)\n","unigram_vectorizer.fit(data_df['tweet_char'])\n","train_unigram_features = unigram_vectorizer.transform(X_train)\n","test_unigram_features = unigram_vectorizer.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ksF-7oHdx4G1"},"source":["Ekstraksi fitur Bigram"]},{"cell_type":"code","metadata":{"id":"zFM0A-xxx4G1"},"source":["bigram_vectorizer = TfidfVectorizer(\n","    sublinear_tf=True,\n","    strip_accents='unicode',\n","    analyzer='word',\n","    token_pattern=r'\\w{1,}',\n","    ngram_range=(1, 2),\n","    max_features=100000)\n","bigram_vectorizer.fit(data_df['tweet_char'])\n","train_bigram_features = bigram_vectorizer.transform(X_train)\n","test_bigram_features = bigram_vectorizer.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"STfZvHVmx4G1"},"source":["Menggabungkan hasil ekstraksi fitur unigram dan bigram"]},{"cell_type":"code","metadata":{"id":"8Sn7kq3Kx4G1"},"source":["train_features = hstack([train_unigram_features, train_bigram_features])\n","test_features = hstack([test_unigram_features, test_bigram_features])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["values, counts = np.unique(y_test, return_counts=True)\n","values\n","counts"],"metadata":{"id":"ui0_DvYQckkc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkPwgaDOIpHg"},"source":["### Proses Ekstraksi Fitur Unigram dan Bigram dengan TF"]},{"cell_type":"markdown","metadata":{"id":"Wix-6wqfJZWz"},"source":["Melakukan split data training dan testing dengan perbandingan  80%:20%"]},{"cell_type":"code","metadata":{"id":"UGWQGLr9JZW0"},"source":["data_df = data_df[(data_df['Sentiment'] != 'Neutral')]\n","X = data_df['tweet_char']\n","y = data_df['Sentiment']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ejAX4TNUIpHr"},"source":["Ekstraksi fitur Unigram"]},{"cell_type":"code","metadata":{"id":"TsinU1gPIpHr"},"source":["unigram_vectorizer = CountVectorizer(\n","    strip_accents='unicode',\n","    analyzer='word',\n","    token_pattern=r'\\w{1,}',\n","    ngram_range=(1, 1),\n","    max_features=100000)\n","unigram_vectorizer.fit(data_df['tweet_char'])\n","train_unigram_features = unigram_vectorizer.transform(X_train)\n","test_unigram_features = unigram_vectorizer.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BGYyq4bTIpHs"},"source":["Ekstraksi fitur Bigram"]},{"cell_type":"code","metadata":{"id":"56xTL8PuIpHs"},"source":["bigram_vectorizer = CountVectorizer(\n","    strip_accents='unicode',\n","    analyzer='word',\n","    token_pattern=r'\\w{1,}',\n","    ngram_range=(1, 2),\n","    max_features=100000)\n","bigram_vectorizer.fit(data_df['tweet_char'])\n","train_bigram_features = bigram_vectorizer.transform(X_train)\n","test_bigram_features = bigram_vectorizer.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fvy7UNS_IpHs"},"source":["Menggabungkan hasil ekstraksi fitur unigram dan bigram"]},{"cell_type":"code","metadata":{"id":"dKpGXDRvIpHs"},"source":["train_features = hstack([train_unigram_features, train_bigram_features])\n","test_features = hstack([test_unigram_features, test_bigram_features])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["values, counts = np.unique(y_test, return_counts=True)\n","values\n","counts"],"metadata":{"id":"hK0JmFH1IpHs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Proses Ekstraksi Fitur dengan Inset Lexicon"],"metadata":{"id":"XcZTKpcoI5Ce"}},{"cell_type":"markdown","source":["Menentukan label tweet dengan InSet Lexicon"],"metadata":{"id":"o1trcqpJMOZ6"}},{"cell_type":"code","source":["lexicon_positive = dict()\n","positive = pd.read_csv('/content/gdrive/MyDrive/Thesis/resources/inSet Lexicon/positive.tsv', sep = '\\t')\n","\n","for i, row in positive.iterrows():\n","  lexicon_positive[row['word']] = int(row['weight'])\n","\n","lexicon_negative = dict()\n","negative = pd.read_csv('/content/gdrive/MyDrive/Thesis/resources/inSet Lexicon/negative.tsv', sep = '\\t')\n","\n","for i, row in negative.iterrows():\n","  lexicon_negative[row['word']] = int(row['weight'])\n","        \n","# Function to determine sentiment polarity of tweets        \n","def sentiment_analysis_lexicon_indonesia(text):\n","    #cleaning text format\n","    #text = text.replace(\"[\", \"\")\n","    #text = text.replace(\"]\", \"\")\n","    #text = text.replace(\"'\", \"\")\n","    #text = text.replace(\" \", \"\")\n","    #text = list(text.split(' '))\n","    #text = [i for i in text if i != '']\n","\n","    #for word in text:\n","    score = 0\n","    for word in text:\n","        if (word in lexicon_positive):\n","            score = score + lexicon_positive[word]\n","    for word in text:\n","        if (word in lexicon_negative):\n","            score = score + lexicon_negative[word]\n","    polarity=''\n","    if (score > 0):\n","        polarity = 'Positive'\n","    elif (score < 0):\n","        polarity = 'Negative'\n","    else:\n","        polarity = 'Neutral'\n","    return score, polarity"],"metadata":{"id":"LkkMs8IfMTU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Results from determine sentiment polarity of tweets\n","\n","results = data_df['tweet_tokens_final'].apply(sentiment_analysis_lexicon_indonesia)\n","results = list(zip(*results))\n","data_df['Polarity_Score'] = results[0]\n","data_df['Polarity'] = results[1]\n","print(data_df['Polarity'].value_counts())"],"metadata":{"id":"HrQUYUmsNv6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["neg0, pos0 = (data_df['Sentiment'][data_df['Sentiment'] == 'Negative']).count(), (data_df['Sentiment'][data_df['Sentiment'] == 'Positive']).count()\n","neg1, pos1 = (data_df['Polarity'][data_df['Polarity'] == 'Negative']).count(), (data_df['Polarity'][data_df['Polarity'] == 'Positive']).count()\n","\n","print('neg:', neg0, '(', '{0:.2f}'.format(neg0/(neg0+pos0)*100), '%)','\\t', 'pos:', pos0, '(', '{0:.2f}'.format(pos0/(neg0+pos0)*100),'%)',' | actual label')\n","print('neg:', neg1, '(', '{0:.2f}'.format(neg1/(neg1+pos1)*100), '%)','\\t', 'pos:', pos1, '(', '{0:.2f}'.format(pos1/(neg1+pos1)*100),'%)',' | inset')"],"metadata":{"id":"bGf9PEkhVxwA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Melakukan tokenisasi"],"metadata":{"id":"NKcxRiH4dFQ3"}},{"cell_type":"code","source":["#Step - a : Menghapus baris kosong, jika ada.\n","#data_df['tweet_char'].dropna(inplace=True)\n","# # Step - b : Mengganti semua teks ke karakter kecil karena 'oke' dan 'OKE' diinterpretasikan berbeda\n","# Corpus['text'] = [entry.lower() for entry in Corpus['text']] # we've done this in '[1] text cleaning.ipynb'\n","# Step - c : Tokenisasi : Setiap kalimat di dalam korpus akan dipecah menjadi daftar kata/string\n","#data_df['tweet_char']= [word_tokenize(entry) for entry in data_df['tweet_char']]\n","\n","\n","for index, entry in enumerate(data_df['tweet_tokens_final']):\n","    # Mendeklarasikan list kosong untuk menyimpan daftar kata yang sesuai dengan aturan yang dibuat\n","    Final_words = []\n","    for word in entry:\n","        # Kondisi di bawah adalah untuk mengecek/mempertimbangkan alfabet saja\n","        if word.isalpha():\n","            word_Final = word\n","            Final_words.append(word_Final)\n","    data_df.loc[index,'tweet_final'] = str(Final_words)"],"metadata":{"id":"XH2UnGgzdEFm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2YyZn87FJbUH"},"source":["Melakukan split data training dan testing dengan perbandingan  80%:20%"]},{"cell_type":"code","metadata":{"id":"jfteFTeKJbUH"},"source":["data_df = data_df[(data_df['Sentiment'] != 'Neutral')]\n","X = data_df['tweet_final']\n","y = data_df['Sentiment']\n","\n","#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n","X_train_inset, X_test_inset, y_train_inset, y_test_inset = train_test_split(data_df['tweet_final'], data_df['Polarity'], test_size=0.2, random_state=42)\n","y_train, y_test = train_test_split(data_df['Sentiment'], test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train_inset.size, X_train_inset.size/(X_test_inset.size + X_train_inset.size),'%','\\n',\n","      X_test_inset.size, X_test_inset.size/(X_test_inset.size + X_train_inset.size),'%')"],"metadata":{"id":"rOuw6SwTaBhl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encoding label menjadi nilai antara 0 and kelas_n-1\n","Encoder = LabelEncoder()\n","y_train_inset = Encoder.fit_transform(y_train_inset)\n","y_test_inset = Encoder.fit_transform(y_test_inset)\n","#y_train = Encoder.fit_transform(y_train)\n","#y_test = Encoder.fit_transform(y_test)"],"metadata":{"id":"m2aDTpkpacja"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Ekstraksi Fitur Inset Lexicon dengan TF-IDF**"],"metadata":{"id":"VjPiiPaZJAU6"}},{"cell_type":"code","source":["Tfidf_vect = TfidfVectorizer()\n","Tfidf_vect.fit(data_df['tweet_final'])\n","\n","X = Tfidf_vect.fit_transform(data_df['tweet_final'])\n","\n","# Transform Train_X dan Test_X ke vektor TF-IDF\n","train_features = Tfidf_vect.transform(X_train_inset)\n","test_features = Tfidf_vect.transform(X_test_inset)"],"metadata":{"id":"NV7qonadJH0H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Proses Translasi Data Indonesia - Inggris"],"metadata":{"id":"7EX8m07iPKea"}},{"cell_type":"code","source":["def token_translated(word):\n","  try:\n","    word_translated = GoogleTranslator(source='id', target='en').translate(word)\n","  except:\n","    word_translated = word\n","\n","  return word_translated.lower()\n","\n","def text_translated(text):\n","\n","    text_translated = []\n","    for word in text:\n","      word_token = token_translated(word) \n","      text_translated.append(word_token)\n","    \n","    return text_translated"],"metadata":{"id":"XDR0YQ5NPCEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#data_df_translated = data_df['tweet_tokens_final'].swifter.apply(text_translated)\n","#data_translated = pd.DataFrame()\n","#data_translated['tweet_tokens_final'] = data_df['tweet_tokens_final'].copy()\n","#data_translated['tweet_tokens_translated'] = data_df_translated.copy()\n","#data_translated.to_csv('/content/gdrive/MyDrive/Thesis/resources/before_covid_tweet_translated.csv')"],"metadata":{"id":"bt9CoNHJPCEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_translated = pd.read_csv('/content/gdrive/MyDrive/Thesis/resources/before_covid_tweet_translated.csv')"],"metadata":{"id":"BR_SpYLWPCEa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Proses Ekstraksi Fitur dengan SentiWordNet Lexicon"],"metadata":{"id":"pkiJ0heGRbSx"}},{"cell_type":"markdown","source":["Membuat function untuk analsis sentimen"],"metadata":{"id":"g7qguY2PSWfs"}},{"cell_type":"code","source":["sentiwordnet_path = \"/content/gdrive/MyDrive/Thesis/resources/SentiWordNet Lexicon/SentiWordNet_3.0.0.txt\""],"metadata":{"id":"MPPTE4MtU4ui"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_line(line):\n","    cols = line.split(\"\\t\")\n","    return cols\n","\n","def get_words(cols):\n","    words_ids = cols[4].split(\" \")\n","    words = [w.split(\"#\")[0] for w in words_ids]\n","    return words\n","\n","def get_positive(cols):\n","    return cols[2]\n","\n","def get_negative(cols):\n","    return cols[3]\n","\n","def get_objective(cols):\n","    return 1 - (float(cols[2]) + float(cols[3]))\n","\n","def get_gloss(cols):\n","    return cols[5]\n","\n","def get_scores_sentiwordnet(sentiword):\n","\n","    f = open(sentiwordnet_path)\n","    totalobject =0.0\n","    count =0.0\n","    totalpositive =0.0\n","    totalnegative =0.0\n","    for line in f:\n","        if not line.startswith(\"#\"):\n","            cols = split_line(line)\n","            words = get_words(cols)\n","           \n","            for word in sentiword:\n","                if word in words:\n","                    if word == \"not\":\n","                        totalobject = totalobject + 0\n","                        totalpositive = totalpositive + 0\n","                        totalnegative = totalnegative + 16\n","                        count =count + 1\n","                    else:\n","\n","                        totalobject = totalobject + get_objective(cols)\n","                        totalpositive = totalpositive + float(get_positive(cols))\n","                        totalnegative = totalnegative + float(get_negative(cols))\n","                        count =count + 1\n","    \n","    polarity=''\n","    score = 0\n","    if count > 0:\n","        if totalpositive > totalnegative :\n","            polarity = 'Positive'\n","            score = totalpositive\n","        elif totalpositive < totalnegative :\n","            polarity = 'Negative'\n","            score = totalnegative\n","        else :\n","            polarity = 'Neutral'\n","            score = totalpositive\n","\n","    return score, polarity"],"metadata":{"id":"tWelwOxxSUIE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menentukan label tweet dengan InSet Lexicon"],"metadata":{"id":"NyijgxvVRbSy"}},{"cell_type":"code","source":["#results = data_translated['tweet_tokens_translated'].swifter.apply(get_scores_sentiwordnet)\n","#sentiwordnet_results = list(zip(*results))\n","#data_sentiwordnet = pd.DataFrame()\n","#data_sentiwordnet['tweet_tokens_final'] = data_df['tweet_tokens_final'].copy()\n","#data_sentiwordnet['tweet_tokens_translated'] = data_translated['tweet_tokens_translated'].copy()\n","#data_sentiwordnet['Polarity_Score'] = sentiwordnet_results[0]\n","#data_sentiwordnet['Polarity'] = sentiwordnet_results[1]\n","#data_sentiwordnet.to_csv('/content/gdrive/MyDrive/Thesis/resources/before_covid_sentiwordnet.csv')"],"metadata":{"id":"jzTk9bUhtoih"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_sentiwordnet = pd.read_csv('/content/gdrive/MyDrive/Thesis/resources/before_covid_sentiwordnet.csv')\n","data_df['Polarity_Score'] = data_sentiwordnet['Polarity_Score'].copy()\n","data_df['Polarity'] = data_sentiwordnet['Polarity'].copy()"],"metadata":{"id":"1J1iLdkfM02z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["neg0, pos0 = (data_df['Sentiment'][data_df['Sentiment'] == 'Negative']).count(), (data_df['Sentiment'][data_df['Sentiment'] == 'Positive']).count()\n","neg1, pos1 = (data_df['Polarity'][data_df['Polarity'] == 'Negative']).count(), (data_df['Polarity'][data_df['Polarity'] == 'Positive']).count()\n","\n","print('neg:', neg0, '(', '{0:.2f}'.format(neg0/(neg0+pos0)*100), '%)','\\t', 'pos:', pos0, '(', '{0:.2f}'.format(pos0/(neg0+pos0)*100),'%)',' | actual label')\n","print('neg:', neg1, '(', '{0:.2f}'.format(neg1/(neg1+pos1)*100), '%)','\\t', 'pos:', pos1, '(', '{0:.2f}'.format(pos1/(neg1+pos1)*100),'%)',' | sentiwordnet')"],"metadata":{"id":"AbwWbi5PRbSy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Melakukan tokenisasi"],"metadata":{"id":"fBrNq6boRbSz"}},{"cell_type":"code","source":["#Step - a : Menghapus baris kosong, jika ada.\n","#data_df['tweet_char'].dropna(inplace=True)\n","# # Step - b : Mengganti semua teks ke karakter kecil karena 'oke' dan 'OKE' diinterpretasikan berbeda\n","# Corpus['text'] = [entry.lower() for entry in Corpus['text']] # we've done this in '[1] text cleaning.ipynb'\n","# Step - c : Tokenisasi : Setiap kalimat di dalam korpus akan dipecah menjadi daftar kata/string\n","#data_df['tweet_char']= [word_tokenize(entry) for entry in data_df['tweet_char']]\n","\n","\n","for index, entry in enumerate(data_df['tweet_tokens_final']):\n","    # Mendeklarasikan list kosong untuk menyimpan daftar kata yang sesuai dengan aturan yang dibuat\n","    Final_words = []\n","    for word in entry:\n","        # Kondisi di bawah adalah untuk mengecek/mempertimbangkan alfabet saja\n","        if word.isalpha():\n","            word_Final = word\n","            Final_words.append(word_Final)\n","    data_df.loc[index,'tweet_final'] = str(Final_words)"],"metadata":{"id":"v_aiufUTRbSz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yQq595f2RbSz"},"source":["Melakukan split data training dan testing dengan perbandingan  80%:20%"]},{"cell_type":"code","metadata":{"id":"TQP_JLOkRbSz"},"source":["data_df = data_df[(data_df['Sentiment'] != 'Neutral')]\n","X = data_df['tweet_final']\n","y = data_df['Sentiment']\n","\n","#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n","X_train_inset, X_test_inset, y_train_inset, y_test_inset = train_test_split(data_df['tweet_final'], data_df['Polarity'], test_size=0.2, random_state=42)\n","y_train, y_test = train_test_split(data_df['Sentiment'], test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train_inset.size, X_train_inset.size/(X_test_inset.size + X_train_inset.size),'%','\\n',\n","      X_test_inset.size, X_test_inset.size/(X_test_inset.size + X_train_inset.size),'%')"],"metadata":{"id":"SP02L1LvRbSz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encoding label menjadi nilai antara 0 and kelas_n-1\n","Encoder = LabelEncoder()\n","y_train_inset = Encoder.fit_transform(y_train_inset)\n","y_test_inset = Encoder.fit_transform(y_test_inset)\n","#y_train = Encoder.fit_transform(y_train)\n","#y_test = Encoder.fit_transform(y_test)"],"metadata":{"id":"R_FhN_uzRbS0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Ekstraksi Fitur Inset Lexicon dengan TF-IDF**"],"metadata":{"id":"JWqL18WdRbS0"}},{"cell_type":"code","source":["Tfidf_vect = TfidfVectorizer()\n","Tfidf_vect.fit(data_df['tweet_final'])\n","\n","X = Tfidf_vect.fit_transform(data_df['tweet_final'])\n","\n","# Transform Train_X dan Test_X ke vektor TF-IDF\n","train_features = Tfidf_vect.transform(X_train_inset)\n","test_features = Tfidf_vect.transform(X_test_inset)"],"metadata":{"id":"-oN3dyiORbS0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Proses Ekstraksi Fitur dengan AFINN Lexicon"],"metadata":{"id":"gQJTSh0jPCEZ"}},{"cell_type":"markdown","source":["Membuat function untuk analsis sentimen"],"metadata":{"id":"HtKWDhH5PCEa"}},{"cell_type":"code","source":["def afinn_sentiment(text):\n","  # compute scores (polarity) and labels\n","  \n","  polarity=''\n","  scores = afn.score(text)\n","  \n","  if scores > 0 :\n","      polarity = 'Positive'\n","  elif scores < 0 :\n","      polarity = 'Negative'\n","  else :\n","      polarity = 'Neutral'\n","\n","  return scores, polarity"],"metadata":{"id":"dJF_IPVwPCEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["afn = Afinn()\n","#results = data_translated['tweet_tokens_translated'].swifter.apply(afinn_sentiment)\n","#afinn_results = list(zip(*results))\n","#data_afinn = pd.DataFrame()\n","#data_afinn['tweet_tokens_final'] = data_df['tweet_tokens_final'].copy()\n","#data_afinn['tweet_tokens_translated'] = data_translated['tweet_tokens_translated'].copy()\n","#data_afinn['Polarity_Score'] = afinn_results[0]\n","#data_afinn['Polarity'] = afinn_results[1]\n","#data_afinn.to_csv('/content/gdrive/MyDrive/Thesis/resources/before_covid_afinn.csv')"],"metadata":{"id":"OwQCFozuPCEb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menentukan label tweet dengan InSet Lexicon"],"metadata":{"id":"r667clGrPCEb"}},{"cell_type":"code","source":["data_afinn = pd.read_csv('/content/gdrive/MyDrive/Thesis/resources/before_covid_afinn.csv')\n","data_df['Polarity_Score'] = data_afinn['Polarity_Score'].copy()\n","data_df['Polarity'] = data_afinn['Polarity'].copy()"],"metadata":{"id":"gEA86lWsPCEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["neg0, pos0 = (data_df['Sentiment'][data_df['Sentiment'] == 'Negative']).count(), (data_df['Sentiment'][data_df['Sentiment'] == 'Positive']).count()\n","neg1, pos1 = (data_df['Polarity'][data_df['Polarity'] == 'Negative']).count(), (data_df['Polarity'][data_df['Polarity'] == 'Positive']).count()\n","\n","print('neg:', neg0, '(', '{0:.2f}'.format(neg0/(neg0+pos0)*100), '%)','\\t', 'pos:', pos0, '(', '{0:.2f}'.format(pos0/(neg0+pos0)*100),'%)',' | actual label')\n","print('neg:', neg1, '(', '{0:.2f}'.format(neg1/(neg1+pos1)*100), '%)','\\t', 'pos:', pos1, '(', '{0:.2f}'.format(pos1/(neg1+pos1)*100),'%)',' | AFINN Lexicon')"],"metadata":{"id":"tWPgVXPyPCEb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Melakukan tokenisasi"],"metadata":{"id":"UmYMb_3GPCEb"}},{"cell_type":"code","source":["#Step - a : Menghapus baris kosong, jika ada.\n","#data_df['tweet_char'].dropna(inplace=True)\n","# # Step - b : Mengganti semua teks ke karakter kecil karena 'oke' dan 'OKE' diinterpretasikan berbeda\n","# Corpus['text'] = [entry.lower() for entry in Corpus['text']] # we've done this in '[1] text cleaning.ipynb'\n","# Step - c : Tokenisasi : Setiap kalimat di dalam korpus akan dipecah menjadi daftar kata/string\n","#data_df['tweet_char']= [word_tokenize(entry) for entry in data_df['tweet_char']]\n","\n","\n","for index, entry in enumerate(data_df['tweet_tokens_final']):\n","    # Mendeklarasikan list kosong untuk menyimpan daftar kata yang sesuai dengan aturan yang dibuat\n","    Final_words = []\n","    for word in entry:\n","        # Kondisi di bawah adalah untuk mengecek/mempertimbangkan alfabet saja\n","        if word.isalpha():\n","            word_Final = word\n","            Final_words.append(word_Final)\n","    data_df.loc[index,'tweet_final'] = str(Final_words)"],"metadata":{"id":"py5bOa_UPCEb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCs_QIFmPCEb"},"source":["Melakukan split data training dan testing dengan perbandingan  80%:20%"]},{"cell_type":"code","metadata":{"id":"8LbncOR0PCEb"},"source":["data_df = data_df[(data_df['Sentiment'] != 'Neutral')]\n","X = data_df['tweet_final']\n","y = data_df['Sentiment']\n","\n","#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n","X_train_afinn, X_test_afinn, y_train_afinn, y_test_afinn = train_test_split(data_df['tweet_final'], data_df['Polarity'], test_size=0.2, random_state=42)\n","y_train, y_test = train_test_split(data_df['Sentiment'], test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train_afinn.size, X_train_afinn.size/(X_test_afinn.size + X_train_afinn.size),'%','\\n',\n","      X_test_afinn.size, X_test_afinn.size/(X_test_afinn.size + X_train_afinn.size),'%')"],"metadata":{"id":"CbSwRdJZPCEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encoding label menjadi nilai antara 0 and kelas_n-1\n","Encoder = LabelEncoder()\n","y_train_afinn = Encoder.fit_transform(y_train_afinn)\n","y_test_afinn = Encoder.fit_transform(y_test_afinn)\n","#y_train = Encoder.fit_transform(y_train)\n","#y_test = Encoder.fit_transform(y_test)"],"metadata":{"id":"0CecGVrOPCEc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Ekstraksi Fitur Inset Lexicon dengan TF-IDF**"],"metadata":{"id":"xp5FRgUqPCEc"}},{"cell_type":"code","source":["Tfidf_vect = TfidfVectorizer()\n","Tfidf_vect.fit(data_df['tweet_final'])\n","\n","X = Tfidf_vect.fit_transform(data_df['tweet_final'])\n","\n","# Transform Train_X dan Test_X ke vektor TF-IDF\n","train_features = Tfidf_vect.transform(X_train_afinn)\n","test_features = Tfidf_vect.transform(X_test_afinn)"],"metadata":{"id":"kfxcFLpOPCEc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Proses Ekstraksi Fitur dengan Liu Lexicon\n","\n"],"metadata":{"id":"SC1YQCDBTa1H"}},{"cell_type":"markdown","source":["Membaca file Liu Lexicon"],"metadata":{"id":"e5h51d3-VLFB"}},{"cell_type":"code","source":["def isNotNull(value):\n","    return value is not None and len(value) > 0\n","\n","dict_pos = []\n","dict_neg = []\n","\n","f = open('/content/gdrive/MyDrive/Thesis/resources/Liu Lexicon/positive-words.txt', 'r', encoding = \"ISO-8859-1\")\n","for line in f:\n","    t = line.strip().lower();\n","    if (isNotNull(t)):\n","        dict_pos.append(t)\n","f.close()\n","\n","f = open('/content/gdrive/MyDrive/Thesis/resources/Liu Lexicon/negative-words.txt', 'r', encoding = \"ISO-8859-1\")\n","for line in f:\n","    t= line.strip().lower();\n","    if (isNotNull(t)):\n","        dict_neg.append(t)\n","f.close()"],"metadata":{"id":"MvTEJj32VOYE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Membuat function untuk analsis sentimen"],"metadata":{"id":"X3xKfwy5Ta1I"}},{"cell_type":"code","source":["def liu_sentiment(text):\n","  \n","  neg_cnt = 0\n","  pos_cnt = 0\n","\n","  for neg in dict_neg:\n","      if (neg in text):\n","          neg_cnt = neg_cnt +1\n","  for pos in dict_pos:\n","      if (pos in text):\n","          pos_cnt = pos_cnt +1\n","  \n","  analysis_sent = pos_cnt - neg_cnt\n","\n","  polarity=''\n","  scores = analysis_sent\n","  \n","  if analysis_sent > 0 :\n","      polarity = 'Positive'\n","  elif analysis_sent < 0 :\n","      polarity = 'Negative'\n","  else :\n","      polarity = 'Neutral'\n","\n","  return scores, polarity"],"metadata":{"id":"cr6APoI5Ta1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#results = data_translated['tweet_tokens_translated'].swifter.apply(liu_sentiment)\n","#liu_results = list(zip(*results))\n","#data_liu = pd.DataFrame()\n","#data_liu['tweet_tokens_final'] = data_df['tweet_tokens_final'].copy()\n","#data_liu['tweet_tokens_translated'] = data_translated['tweet_tokens_translated'].copy()\n","#data_liu['Polarity_Score'] = liu_results[0]\n","#data_liu['Polarity'] = liu_results[1]\n","#data_liu.to_csv('/content/gdrive/MyDrive/Thesis/resources/before_covid_liu.csv')"],"metadata":{"id":"Rz6e2y8GTa1I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menentukan label tweet dengan InSet Lexicon"],"metadata":{"id":"5Ib7gk68Ta1I"}},{"cell_type":"code","source":["data_liu = pd.read_csv('/content/gdrive/MyDrive/Thesis/resources/before_covid_liu.csv')\n","data_df['Polarity_Score'] = data_liu['Polarity_Score'].copy()\n","data_df['Polarity'] = data_liu['Polarity'].copy()"],"metadata":{"id":"UupLnot_Ta1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["neg0, pos0 = (data_df['Sentiment'][data_df['Sentiment'] == 'Negative']).count(), (data_df['Sentiment'][data_df['Sentiment'] == 'Positive']).count()\n","neg1, pos1 = (data_df['Polarity'][data_df['Polarity'] == 'Negative']).count(), (data_df['Polarity'][data_df['Polarity'] == 'Positive']).count()\n","\n","print('neg:', neg0, '(', '{0:.2f}'.format(neg0/(neg0+pos0)*100), '%)','\\t', 'pos:', pos0, '(', '{0:.2f}'.format(pos0/(neg0+pos0)*100),'%)',' | actual label')\n","print('neg:', neg1, '(', '{0:.2f}'.format(neg1/(neg1+pos1)*100), '%)','\\t', 'pos:', pos1, '(', '{0:.2f}'.format(pos1/(neg1+pos1)*100),'%)',' | Liu Lexicon')"],"metadata":{"id":"Dybcql8sTa1I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Melakukan tokenisasi"],"metadata":{"id":"Qn3B9Ko4Ta1J"}},{"cell_type":"code","source":["#Step - a : Menghapus baris kosong, jika ada.\n","#data_df['tweet_char'].dropna(inplace=True)\n","# # Step - b : Mengganti semua teks ke karakter kecil karena 'oke' dan 'OKE' diinterpretasikan berbeda\n","# Corpus['text'] = [entry.lower() for entry in Corpus['text']] # we've done this in '[1] text cleaning.ipynb'\n","# Step - c : Tokenisasi : Setiap kalimat di dalam korpus akan dipecah menjadi daftar kata/string\n","#data_df['tweet_char']= [word_tokenize(entry) for entry in data_df['tweet_char']]\n","\n","\n","for index, entry in enumerate(data_df['tweet_tokens_final']):\n","    # Mendeklarasikan list kosong untuk menyimpan daftar kata yang sesuai dengan aturan yang dibuat\n","    Final_words = []\n","    for word in entry:\n","        # Kondisi di bawah adalah untuk mengecek/mempertimbangkan alfabet saja\n","        if word.isalpha():\n","            word_Final = word\n","            Final_words.append(word_Final)\n","    data_df.loc[index,'tweet_final'] = str(Final_words)"],"metadata":{"id":"pwN2CNZfTa1J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"218zUBnrTa1J"},"source":["Melakukan split data training dan testing dengan perbandingan  80%:20%"]},{"cell_type":"code","metadata":{"id":"DwmpxNZ4Ta1J"},"source":["data_df = data_df[(data_df['Sentiment'] != 'Neutral')]\n","X = data_df['tweet_final']\n","y = data_df['Sentiment']\n","\n","#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n","X_train_liu, X_test_liu, y_train_liu, y_test_liu = train_test_split(data_df['tweet_final'], data_df['Polarity'], test_size=0.2, random_state=42)\n","y_train, y_test = train_test_split(data_df['Sentiment'], test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(X_train_liu.size, X_train_liu.size/(X_test_liu.size + X_train_liu.size),'%','\\n',\n","      X_test_liu.size, X_test_liu.size/(X_test_liu.size + X_train_liu.size),'%')"],"metadata":{"id":"cddxRP2wTa1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Encoding label menjadi nilai antara 0 and kelas_n-1\n","Encoder = LabelEncoder()\n","y_train_liu = Encoder.fit_transform(y_train_liu)\n","y_test_liu = Encoder.fit_transform(y_test_liu)\n","#y_train = Encoder.fit_transform(y_train)\n","#y_test = Encoder.fit_transform(y_test)"],"metadata":{"id":"vLpXh6c4Ta1J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Ekstraksi Fitur Inset Lexicon dengan TF-IDF**"],"metadata":{"id":"A8lU_5A2Ta1J"}},{"cell_type":"code","source":["Tfidf_vect = TfidfVectorizer()\n","Tfidf_vect.fit(data_df['tweet_final'])\n","\n","X = Tfidf_vect.fit_transform(data_df['tweet_final'])\n","\n","# Transform Train_X dan Test_X ke vektor TF-IDF\n","train_features = Tfidf_vect.transform(X_train_liu)\n","test_features = Tfidf_vect.transform(X_test_liu)"],"metadata":{"id":"tVce5IRLTa1J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-sCbNWFfx4G2"},"source":["\n","### Pemodelan dan Evaluasi"]},{"cell_type":"markdown","metadata":{"id":"dVIpc_sTx4G2"},"source":["Naive Bayes"]},{"cell_type":"code","metadata":{"id":"pUw1Otiyx4G2"},"source":["cNB = GaussianNB()\n","modelNB = cNB.fit(train_features.toarray(), y_train.tolist())\n","\n","predictNB = modelNB.predict(test_features.toarray())\n","print(classification_report(y_test.tolist(), predictNB, labels=['Positive', 'Negative']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-eKZuBXYx4G3"},"source":["Linear Regression"]},{"cell_type":"code","source":["cLR = LogisticRegression(random_state=77)\n","modelLR = cLR.fit(train_features, y_train)\n","\n","predictLR = modelLR.predict(test_features)\n","print(classification_report(y_test, predictLR, labels=['Positive', 'Negative']))"],"metadata":{"id":"b6wdATT81tm4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZBr2yQLbx4G4"},"source":["Linear SVM"]},{"cell_type":"code","metadata":{"id":"dTMVMxEIx4G5"},"source":["cSVM = svm.SVC()\n","modelSVM = cSVM.fit(train_features, y_train)\n","\n","predictSVM = modelSVM.predict(test_features)\n","print(classification_report(y_test, predictSVM, labels=['Positive', 'Negative']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sInwoy0Wx4G5"},"source":["MLP Classifier"]},{"cell_type":"code","metadata":{"id":"8DV85rsix4G5"},"source":["cMLP = MLPClassifier(random_state=1, max_iter=500)\n","modelMLP = cMLP.fit(train_features, y_train)\n","\n","predictMLP = modelMLP.predict(test_features)\n","print(classification_report(y_test, predictMLP, labels=['Positive', 'Negative']))"],"execution_count":null,"outputs":[]}]}