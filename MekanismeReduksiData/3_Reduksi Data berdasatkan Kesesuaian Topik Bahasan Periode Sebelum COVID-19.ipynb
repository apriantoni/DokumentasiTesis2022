{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat Sentiment Analysis dari Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Persiapan Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Packages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada pengerjaan ini, ada beberapa packages yang digunakan yaitu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-pcre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mysql-connector-python-rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda update anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install little-mallet-wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melakukan Import Library**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pertama, lakukan import library yang dibutuhkan berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#import libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string \n",
    "import re\n",
    "import spacy\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import collections\n",
    "import datetime\n",
    "import lemmatizer as lemma\n",
    "import mysql.connector\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import nltk\n",
    "\n",
    "%matplotlib inline\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import STOPWORDS\n",
    "from wordcloud import WordCloud \n",
    "from nltk.corpus import stopwords\n",
    "from operator import itemgetter\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from mysql.connector import errorcode\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "\n",
    "# import Sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "\n",
    "#import gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, lakukan download resources dari NLTK untuk proses text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persiapan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deklarasi date_start dan date_end untuk masing-masing periode sebelum dan saat COVID-19 memetakan label setiap periodenya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "date_start = date(2019,12,13)\n",
    "date_end   = date(2020,6,13)\n",
    "\n",
    "date_start_convert = date_start.strftime('%d%m%Y')\n",
    "date_end_convert = date_end.strftime('%d%m%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mengambil Data Retweets Pengguna**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lakukan load data retweets beserta data username penggunanya dari file CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file CSV\n",
    "retweets_df = pd.read_csv('data/interractions/noncovid/retweets_data_after_joining_13122019-13062020.csv')\n",
    "retweets_copy_df = retweets_df[['tweet_id','conversation_id','username', 'user_id', 'tweet', 'retweet', 'date_created', 'language']].copy()\n",
    "retweets_copy_df['date_created']= pd.to_datetime(retweets_copy_df['date_created'])\n",
    "retweets_copy_df['date'] = retweets_copy_df['date_created'].dt.date\n",
    "retweets_copy_df['time'] = retweets_copy_df['date_created'].dt.time\n",
    "del retweets_copy_df[\"date_created\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mengambil Data Tweets Pengguna**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lakukan load data tweets beserta data username penggunanya dari file CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toniafriantoni/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0,1,2,16,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#read file CSV\n",
    "tweets_during_covid_df = pd.read_csv('data/filter/noncovid/tweet_after_filter_13122019-13032020.csv')\n",
    "tweets_during_covid_copy_df = tweets_during_covid_df[['id','conversation_id','username', 'user_id', 'tweet', 'retweet', 'date', 'time', 'language']].copy()\n",
    "tweets_during_covid_copy_df = tweets_during_covid_copy_df.rename(columns={'id': 'tweet_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Menggabungkan data Retweets dengan Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lakukan penggabungan data retweets dan tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Data:  305165\n"
     ]
    }
   ],
   "source": [
    "#gabungkan data\n",
    "result_data_during_covid = pd.concat([tweets_during_covid_copy_df, retweets_copy_df])\n",
    "#ganti value 1 menjadi True pada kolom retweet\n",
    "result_data_during_covid['retweet'] = result_data_during_covid['retweet'].replace([1],'True')\n",
    "#menghapus dupicat kolom\n",
    "result_data_during_covid=result_data_during_covid.loc[:, ~result_data_during_covid.columns[::-1].duplicated()[::-1]]\n",
    "\n",
    "print(\"Jumlah Data: \", len(result_data_during_covid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Menyeleksi Data Tweet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menyeleksi dan memilih data tweet yang memiliki jumlah kata lebih dari 5. INi sesuai dari referesi artikel pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_tweet(tweet_data_df):\n",
    "    #memilih data tweet yang berbahasa indonesia\n",
    "    tweet_data_df = tweet_data_df[(tweet_data_df['language'] == 'in')]\n",
    "    \n",
    "    #menghapus karakter spesial dari data tweet\n",
    "    case_folding_result = case_folding(tweet_data_df)\n",
    "    special_char_result = special_char(case_folding_result)\n",
    "    \n",
    "    #memilih data tweet yang memiliki lebih dari 5 kata\n",
    "    tweet_data_df['tweet_length'] = [len(x.split()) for x in special_char_result['tweet_char'].tolist()]\n",
    "    tweet_data_df = tweet_data_df[(tweet_data_df['tweet_length']>=5)]\n",
    "    \n",
    "    return tweet_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Melakukan Case Folding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Case folding* bertujuan untuk mengubah semua huruf dalam sebuah dokumen teks menjadi huruf kecil (lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gunakan fungsi Series.str.lower() pada Pandas\n",
    "def case_folding(tweet_data_df):\n",
    "    tweet_data_df['tweet_char'] = tweet_data_df['tweet'].str.lower()\n",
    "    \n",
    "    return tweet_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Menghapus Spesial Karakter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses ini digunakan untuk menghapus number, link, punctuation, single char, space, entities, mention, hastag dan karakter spesial lain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:14: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:31: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:62: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:3: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:14: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:31: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:62: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:3: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:14: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:31: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:62: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-28-1a33ec4cff09>:3: DeprecationWarning: invalid escape sequence \\.\n",
      "  text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text)\n",
      "<ipython-input-28-1a33ec4cff09>:14: DeprecationWarning: invalid escape sequence \\s\n",
      "  text = re.sub('@[^\\s]+[ \\t]','',text)\n",
      "<ipython-input-28-1a33ec4cff09>:31: DeprecationWarning: invalid escape sequence \\.\n",
      "  text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ', text)\n",
      "<ipython-input-28-1a33ec4cff09>:62: DeprecationWarning: invalid escape sequence \\s\n",
      "  return re.sub('\\s+',' ',text)\n"
     ]
    }
   ],
   "source": [
    "def remove_tweet_special(text):\n",
    "    # Remove every URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text)\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # remove non ASCII (emoticon, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove incomplete URL\n",
    "    text = text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "    # Remove every retweet symbol\n",
    "    text = re.sub('(?i)rt',' ',text)\n",
    "    # Remove every username\n",
    "    text = re.sub('@[^\\s]+[ \\t]','',text)\n",
    "     # Remove every username\n",
    "    text = re.sub('(?i)user','',text)\n",
    "    # Remove every url\n",
    "    text = re.sub('(?i)url',' ',text)\n",
    "    # Remove every emoji\n",
    "    text = re.sub(r'\\\\x..',' ',text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub('  +', ' ', text)\n",
    "    #Remove characters repeating more than twice\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text) \n",
    "    \n",
    "    return text\n",
    "\n",
    "def strip_links(text):\n",
    "    # remove link\n",
    "    text = re.sub(r'pic.twitter.com.[\\w]+', '', text) # Remove every pic \n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "def special_char(tweet_data_df):\n",
    "    tweet_data_df['tweet_char'] = tweet_data_df['tweet_char'].swifter.apply(remove_tweet_special)\n",
    "    tweet_data_df['tweet_char'] = tweet_data_df['tweet_char'].swifter.apply(strip_links)\n",
    "    tweet_data_df['tweet_char'] = tweet_data_df['tweet_char'].swifter.apply(strip_all_entities)\n",
    "    tweet_data_df['tweet_char'] = tweet_data_df['tweet_char'].swifter.apply(remove_number)\n",
    "    tweet_data_df['tweet_char'] = tweet_data_df['tweet_char'].swifter.apply(remove_punctuation)\n",
    "    tweet_data_df['tweet_char'] = tweet_data_df['tweet_char'].swifter.apply(remove_whitespace_LT)\n",
    "    tweet_data_df['tweet_char'] = tweet_data_df['tweet_char'].swifter.apply(remove_whitespace_multiple)\n",
    "    tweet_data_df['tweet_char'] = tweet_data_df['tweet_char'].swifter.apply(remove_singl_char)\n",
    "    \n",
    "    return tweet_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tokenizing* adalah operasi memisahkan teks menjadi potongan-potongan berupa token, bisa berupa potongan huruf, kata, atau kalimat, sebelum dianalisis lebih lanjut. Entitas yang bisa disebut sebagai token misalnya kata, angka, simbol, tanda baca, dan lain sebagainya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK word rokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "    \n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "# create N-grams\n",
    "def make_n_grams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)  # higher threshold fewer phrases.\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    bigrams_text = [bigram_mod[doc] for doc in texts]\n",
    "    trigrams_text =  [trigram_mod[bigram_mod[doc]] for doc in bigrams_text]\n",
    "    return trigrams_text\n",
    "\n",
    "def first_step_tokenizing(tweet_data_df):\n",
    "    #tweet_data_df['tweet_tokens'] = tweet_data_df['tweet'].apply(word_tokenize_wrapper)\n",
    "    tweet_data_df['tweet_tokens'] = list(sent_to_words(tweet_data_df['tweet_char']))\n",
    "    tweet_data_df['tweet_tokens'] = make_n_grams(tweet_data_df['tweet_tokens'])\n",
    "    \n",
    "    return tweet_data_df\n",
    "\n",
    "def second_step_tokenizing(tweet_data_df):\n",
    "    #tweet_data_df['tweet_tokens'] = tweet_data_df['tweet'].apply(word_tokenize_wrapper)\n",
    "    tweet_data_df['tweet_tokens2'] = list(sent_to_words(tweet_data_df['tweet_normalized']))\n",
    "    tweet_data_df['tweet_tokens2'] = make_n_grams(tweet_data_df['tweet_tokens2'])\n",
    "    \n",
    "    return tweet_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Menghitung Frekwensi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan perhitungan frekuensi distribusi token pada tiap row data pada Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK calc frequency distribution\n",
    "def freqDist_wrapper(text):\n",
    "    return FreqDist(text)\n",
    "\n",
    "def count_frequency(tweet_data_df):\n",
    "    tweet_data_df['tweet_tokens_fdist'] = tweet_data_df['tweet_tokens'].swifter.apply(freqDist_wrapper)\n",
    "\n",
    "    return tweet_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering (Stopword Removal)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menghilangkan kata umum (*common words*) yang biasanya muncul dalam jumlah besar dan dianggap tidak memiliki makna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "# append additional stopword\n",
    "#list_stopwords.extend(['test'])\n",
    "\n",
    "# read txt stopword using pandas\n",
    "\n",
    "txt_stopword1 = pd.read_csv(\"KK/Final Project/resources/stopwords/stopwords.txt\", names= [\"stopwords\"], header = None)\n",
    "txt_stopword2 = pd.read_csv(\"KK/Final Project/resources/stopwords/stopword2.txt\", names= [\"stopwords\"], header=None)\n",
    "txt_stopword3 = pd.read_csv(\"KK/Final Project/resources/unimportant text/unimportant-text.txt\", names= [\"stopwords\"], header=None)\n",
    "txt_stopword4 = pd.read_csv(\"KK/Final Project/resources/stopwords/stopwords4.csv\", names= [\"stopwords\"], header = None)\n",
    "txt_stopword5 = pd.read_csv(\"KK/Final Project/resources/unimportant text/other-unimportant-text.txt\", names= [\"stopwords\"], header=None)\n",
    "\n",
    "txt_stopword1 = txt_stopword1[\"stopwords\"][0].split(' ')\n",
    "txt_stopword2 = txt_stopword2[\"stopwords\"][0].split(' ')\n",
    "txt_stopword3 = txt_stopword3[\"stopwords\"][0].split(' ')\n",
    "txt_stopword4 = txt_stopword4[\"stopwords\"][0].split(' ')\n",
    "txt_stopword5 = txt_stopword5[\"stopwords\"][0].split(' ')\n",
    "        \n",
    "# convert stopword string to list & append additional stopword\n",
    "list_stopwords.extend(txt_stopword1)\n",
    "list_stopwords.extend(txt_stopword2)\n",
    "list_stopwords.extend(txt_stopword3)\n",
    "list_stopwords.extend(txt_stopword4)\n",
    "list_stopwords.extend(txt_stopword5)\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    words = ast.literal_eval(words)\n",
    "    \n",
    "    #filtered_sentence = [word for word in words if not word in list_stopwords]\n",
    "    filtered_sentence = [] \n",
    "    \n",
    "    for word in list(words):\n",
    "        tokens = word.split(' ')\n",
    "        \n",
    "        if len(tokens)> 1:\n",
    "            for token in list(tokens):\n",
    "                if token not in list_stopwords: \n",
    "                    filtered_sentence.append(token) \n",
    "            \n",
    "        else:\n",
    "            if word not in list_stopwords: \n",
    "                filtered_sentence.append(word) \n",
    "    \n",
    "    return filtered_sentence\n",
    "        \n",
    "#function\n",
    "def first_step_stopwords_process(tweet_data_df):\n",
    "    tweet_data_df['tweet_tokens_WSW'] = (tweet_data_df['tweet_post_tag'].apply(stopwords_removal))\n",
    "\n",
    "    return tweet_data_df\n",
    "\n",
    "#function\n",
    "def second_step_stopwords_process(tweet_data_df):\n",
    "    tweet_data_df['tweet_tokens_final'] = (tweet_data_df['tweet_tokens_lemma'].swifter.apply(stopwords_removal))\n",
    "\n",
    "    return tweet_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization digunakan untuk menyeragamkan term yang memiliki makna sama namun penulisanya berbeda, bisa diakibatkan kesalahan penulisan, penyingkatan kata, ataupun “bahasa gaul”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file1\n",
    "normalizad_word1 = pd.read_csv(\"KK/Final Project/resources/slang words/colloquial-indonesian-lexicon.csv\")\n",
    "#file2\n",
    "normalizad_word2 = pd.read_csv(\"KK/Final Project/resources/slang words/kamusalay.csv\", header=None)\n",
    "#file3\n",
    "file = open(\"KK/Final Project/resources/slang words/combined_slang_words.txt\", \"r\")\n",
    "contents = file.read()\n",
    "normalizad_word3 = ast.literal_eval(contents)\n",
    "file.close()\n",
    "#file4\n",
    "normalizad_word4 = pd.read_csv(\"KK/Final Project/resources/slang words/new_kamusalay.csv\", header=None)\n",
    "#file5\n",
    "normalizad_word5 = pd.read_csv('KK/Final Project/resources/slang words/kbba.txt', sep=\"\\t\", header=None)\n",
    "\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word1.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1] \n",
    "        \n",
    "for index, row in normalizad_word2.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1] \n",
    "        \n",
    "for index in normalizad_word3.keys():\n",
    "    if index not in normalizad_word_dict:\n",
    "        normalizad_word_dict[index] = normalizad_word3[index]\n",
    "\n",
    "for index, row in normalizad_word4.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1]\n",
    "        \n",
    "for index, row in normalizad_word5.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1]\n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "def normalization(tweet_data_df):\n",
    "    tweet_data_df['tweet_normalized'] = tweet_data_df['tweet_tokens'].swifter.apply(normalized_term)\n",
    "\n",
    "    #tweet_data_df['tweet_normalized'].head(10)\n",
    "    \n",
    "    return tweet_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function stemmer dari library Sastrawi untuk mengembalikan kata kebentuk dasarnya. Karena fungsi stemmer.stem() pada library Sastrawi lambat, kita dapat menggunakan library swifter untuk mempercepat froses stemming pada Dataframe dengan menjalankan task secara parallel. Kecepatan pemrosesan bisa dua kali bahkan lebih cepat jika tanpa menggunakan swifter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "def stemming(tweet_data_df):\n",
    "    for document in tweet_data_df['tweet_tokens_WSW']:\n",
    "        for term in document:\n",
    "            if term not in term_dict:\n",
    "                term_dict[term] = ' '\n",
    "\n",
    "    for term in term_dict:\n",
    "        term_dict[term] = stemmed_wrapper(term)\n",
    "        #print(term,\":\" ,term_dict[term])\n",
    "\n",
    "    tweet_data_df['tweet_tokens_stemmed'] = tweet_data_df['tweet_tokens_WSW'].swifter.apply(get_stemmed_term)\n",
    "    #print(tweet_data_df['tweet_tokens_stemmed'])\n",
    "\n",
    "    return  tweet_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_process(tweet_data_df):\n",
    "    lem = lemma.Lemmatizer()\n",
    "    _stopwords_ = stopwords.words('indonesian')\n",
    "    preprocessed_sent = []\n",
    "\n",
    "    lemmatized_sent = []\n",
    "    \n",
    "    tweet_data_df = tweet_data_df.replace(\"['\",'')\n",
    "    tweet_data_df = tweet_data_df.replace(\"']\",'')\n",
    "    tweet_data_df = tweet_data_df.replace(\"'\",'')\n",
    "    \n",
    "    tweet_data_df = tweet_data_df.split(',')\n",
    "    \n",
    "    for token in tweet_data_df:\n",
    "        lemmatized = lem.lemmatize(token)\n",
    "        \n",
    "        if(type(lemmatized) == tuple):\n",
    "            lemmatized_sent.append(lemmatized[0])\n",
    "        else:\n",
    "            lemmatized_sent.append(lemmatized)\n",
    "            \n",
    "    return lemmatized_sent\n",
    "    \n",
    "def lemmatization(tweet_data_df):\n",
    "            \n",
    "    tweet_data_df['tweet_tokens_lemma'] = tweet_data_df['tweet_tokens_stemmed'].swifter.apply(lemmatization_process)\n",
    "\n",
    "    return  tweet_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "corpus = NLPTaskDataFetcher.load_corpus(NLPTask.UD_INDONESIAN)\n",
    "\n",
    "tag_type = 'upos'\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, BertEmbeddings\n",
    "from typing import List\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "embedding_types: List[TokenEmbeddings] = [ \n",
    "    WordEmbeddings('id-crawl'),\n",
    "    WordEmbeddings('id'),\n",
    "]\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "from flair.models import SequenceTagger\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                      tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "\n",
    "tag_pos = SequenceTagger.load('resources/taggers/example-universal-pos/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptional_word = ['corona','coronavirus','covid','ncov','pandemi','lockdown','psbb','sars-cov-2', 'covid-19', 'covid-19']\n",
    "\n",
    "def post_tag_filter(tweet_data_df):\n",
    "    \n",
    "    sentence = Sentence(tweet_data_df)\n",
    "    tag_pos.predict(sentence)\n",
    "    words = sentence.to_dict(tag_type='upos')\n",
    "    \n",
    "    post_tag = []\n",
    "\n",
    "    for i in range(0, len(sentence)):\n",
    "        \n",
    "        word = str(words['entities'][i]['labels'][0]).split(' ')\n",
    "        \n",
    "        if words['entities'][i]['text'] in list(exceptional_word):\n",
    "            post_tag.append(words['entities'][i]['text'])\n",
    "        elif (word[0] == \"NOUN\" or word[0] == \"ADJ\" or word[0] == \"VERB\" or word[0] == \"ADV\"):\n",
    "            post_tag.append(words['entities'][i]['text'])\n",
    "\n",
    "    return post_tag\n",
    "    \n",
    "    \n",
    "def post_tag(tweet_data_df):\n",
    "            \n",
    "    tweet_data_df['tweet_post_tag'] = tweet_data_df['tweet_tokens2'].swifter.apply(post_tag_filter)\n",
    "\n",
    "    return  tweet_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pemilihan Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eksekusi Text Processing** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def text_processing(tweet_data_df, text_name):\n",
    "    \n",
    "    #memanggil function untuk text processing\n",
    "    case_folding_result = case_folding(tweet_data_df)\n",
    "    special_char_result = special_char(case_folding_result)\n",
    "    first_tokenizing_result = first_step_tokenizing(special_char_result)\n",
    "    count_frequency_result = count_frequency(first_tokenizing_result)\n",
    "    normalization_result = normalization(count_frequency_result)\n",
    "    second_tokenizing_result = second_step_tokenizing(normalization_result)\n",
    "    post_tag_result = post_tag(second_tokenizing_result)\n",
    "    post_tag_result.to_csv('data/wordcloud/noncovid/post_tag_result_{}.csv'.format(len(tweet_data_df)))\n",
    "    first_stopwords_result = first_step_stopwords_process(post_tag_result)\n",
    "    stemming_result = stemming(first_stopwords_result)\n",
    "    stemming_result.to_csv('data/wordcloud/noncovid/stemming_result_{}.csv'.format(len(tweet_data_df)))\n",
    "    lemmatization_result = lemmatization(stemming_result)\n",
    "    lemmatization_result.to_csv('data/wordcloud/noncovid/lemmatization_result_{}.csv'.format(len(tweet_data_df)))\n",
    "    second_stopwords_result = second_step_stopwords_process(lemmatization_result)\n",
    "    #check_word_result = check_word(second_stopwords_result)\n",
    "    \n",
    "    \n",
    "    #menyimpan hasil processing ke file *.txt\n",
    "    #deklarasi nama file untuk menyimpan hasil normalisasi\n",
    "    file = 'output/teks_string_{}.txt'.format(text_name)\n",
    "    \n",
    "    #menghapus file agar tidak duplikasi\n",
    "    #with open(file, 'w'): pass\n",
    "    \n",
    "    #Membuat file penyimpanan hasil normalisasi ke file.txt\n",
    "    try:\n",
    "        #create folder data before covid-19\n",
    "        if not os.path.exists(file):\n",
    "            f= open(file, \"w+\")\n",
    "\n",
    "    except:\n",
    "        print('Error')\n",
    "        \n",
    "    return (second_stopwords_result, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Menampilkan WordCloud**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memanggil fungsi untuk konversi Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(tweet_data_df, file_name):\n",
    "    #tweet_data_df['tweet_tokens_lemma'] = tweet_data_df['tweet_tokens_lemma'].apply(lambda x: x[1:-1].split(','))\n",
    "    for i, row in tweet_data_df['tweet_tokens_final'].iteritems():\n",
    "        #tweets_string.append(str)\n",
    "        f=open(file_name, \"a+\")\n",
    "        f.write(\" {}\".format(row))\n",
    "    \n",
    "    with open(file_name, 'r') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "    \n",
    "    data = data.replace(',', '')\n",
    "    data = data.replace(' ', ',')\n",
    "    data = data.replace(',,,', ',')\n",
    "    data = data.replace(',,', ',')\n",
    "    data = data.replace('[]', '')\n",
    "    data = data.replace('[', '')\n",
    "    data = data.replace(']', '')\n",
    "    data = data.replace(\"'\",'')\n",
    "    \n",
    "    #menghilangkan kata yang gagal terhapus di proses stopword sebelumnya\n",
    "    querywords = data.split(\",\")\n",
    "    resultwords  = [word for word in querywords if word.lower() not in list_stopwords]\n",
    "    result = ' '.join(resultwords)\n",
    "    \n",
    "    wc = WordCloud(background_color=\"white\", max_words=100,\n",
    "               stopwords=STOPWORDS, max_font_size=256,\n",
    "               random_state=42, width=800, height=400)\n",
    "    wc.generate(result)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(wc,interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pemanggilan Fungsi Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#melakukan seleksi data\n",
    "selected_data = selected_tweet(result_data_during_covid)\n",
    "\n",
    "#melakukan text processing\n",
    "text_result = text_processing(final_selected_data, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('data/wordcloud/noncovid/lemmatization/lemmatization_result_152432.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#menampilkan wordcloud\n",
    "print('Hasil Wordcloud')\n",
    "print('')\n",
    "result = wordcloud(file, 'teks_string_before_covid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e2c1f1666c461188750c0615325b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Pandas Apply'), FloatProgress(value=0.0, max=152432.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_data_df = file.copy()\n",
    "second_stopwords_result = second_step_stopwords_process(tweet_data_df)\n",
    "second_stopwords_result.to_csv('data/wordcloud/noncovid/final result/final_result_{}.csv'.format(len(second_stopwords_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = [0]\n",
    "#file.drop(file.columns[cols], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file.to_csv('data/wordcloud/covid/final result/final_result_{}.csv'.format(len(file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = text_result[0]['tweet_tokens_lemma'].copy()\n",
    "#data_input = data_input.str.split(',')\n",
    "data_input = data_input.replace(',','', regex=True)\n",
    "data_input = data_input.str.split()\n",
    "data_input = data_input.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_input)\n",
    "texts = data_input\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "#[[(id2word[id], freq) for id, freq in cp] for cp in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "num_topics = 10\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                   id2word=id2word,\n",
    "                   num_topics=10, \n",
    "                   random_state=0,\n",
    "                   chunksize=100,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)\n",
    "\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_input, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = 'mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=5, id2word=id2word)\n",
    "\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_input, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "#model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_input, start=2, limit=40, step=4)\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_input, start=2, limit=10, step=1)\n",
    "\n",
    "# Show graph\n",
    "limit=40; start=2; step=4;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result_index = coherence_values.index(max(coherence_values))\n",
    "optimal_model = model_list[best_result_index]\n",
    "# Select the model and print the topics\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "print(f'''The {x[best_result_index]} topics gives the highest coherence score \\\\\n",
    "of {coherence_values[best_result_index]}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertldaGenToldaMallet(mallet_model):\n",
    "    model_gensim = LdaModel(\n",
    "        id2word=mallet_model.id2word, num_topics=mallet_model.num_topics,\n",
    "        alpha=mallet_model.alpha, eta=0,\n",
    "    )\n",
    "    model_gensim.state.sstats[...] = mallet_model.wordtopics\n",
    "    model_gensim.sync_state()\n",
    "    return model_gensim\n",
    "\n",
    "optimal_model = convertldaGenToldaMallet(optimal_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "#Creating Topic Distance Visualization \n",
    "pyLDAvis.enable_notebook()\n",
    "p = pyLDAvis.gensim.prepare(optimal_model, corpus, id2word)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# feed the LDA model into the pyLDAvis instance\n",
    "lda_viz = gensimvis.prepare(optimal_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_reduksi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduksi = text_result[0].head(50).copy()\n",
    "data_test = data_reduksi['tweet']\n",
    "#data_input = data_input.str.split(',')\n",
    "data_test = data_test.replace(',','', regex=True)\n",
    "data_test = data_test.str.split()\n",
    "data_test = data_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_test = corpora.Dictionary(data_test)\n",
    "texts_test = data_input\n",
    "corpus_test = [id2word_test.doc2bow(text) for text in texts_test]\n",
    "\n",
    "print(corpus_test[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model[corpus_test[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus_test, texts=texts_test):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,2), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus_test, texts=texts_test)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyLDAvis\n",
    "#!pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# feed the LDA model into the pyLDAvis instance\n",
    "lda_viz = gensimvis.prepare(lda_model2, corpus, id2word)\n",
    "lda_viz\n",
    "\n",
    "#pyLDAvis.save_html(lda_viz, 'lda.html')\n",
    "pyLDAvis.display(lda_viz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
